{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 다대일 RNN 기본 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu와 cuda 중 다음 기기로 학습함: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ce2bd746d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets\n",
    "import random\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", DEVICE)\n",
    "\n",
    "SEED = 5\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-1. Review Data 수집 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('IMDb_Reviews.csv', <http.client.HTTPMessage at 0x28a2c456d08>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수 : 50000\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 개수 : {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample 수가 많으므로, Test용으로 대폭 줄여 새로운 파일을 만든 후 사용\n",
    "df = pd.read_csv('IMDb_Reviews_200.csv', encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수 : 200\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 개수 : {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 데이터, 평가 데이터, 테스트 데이터로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[:150]\n",
    "test_df = df[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_data.csv\", index=False)\n",
    "test_df.to_csv(\"test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-2. Field 정의 및 Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True,lower=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = TabularDataset.splits(\n",
    "        path='.', train='train_data.csv', test='test_data.csv', format='csv',\n",
    "        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)    # field: Data column format 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset sample 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 개수 : 150\n",
      "테스트 샘플의 개수 : 50\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플의 개수 : {}'.format(len(trainset)))\n",
    "print('테스트 샘플의 개수 : {}'.format(len(testset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x000001CE2FB64548>\n"
     ]
    }
   ],
   "source": [
    "print(trainset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['i', 'have', 'seen', 'this', 'movie', 'maybe', 'a', '100', 'times,', 'never', 'grow', 'tired', 'of', 'it.i', 'saw', 'this', 'movie', 'the', 'first', 'time', 'when', 'i', 'was', '7', 'years', 'old,', 'and', 'it', 'has', 'left', 'a', 'mark', 'in', 'my', 'memories', 'since', 'then.', 'its', 'a', 'enchanting', 'love', 'story', 'that', 'brings', 'the', 'sun', 'out', 'in', 'most', 'people,', 'even', 'in', 'the', 'darkest', 'times.', 'i', 'think', 'that', 'this', 'is', 'a', '\"must', 'see\"', 'movie', 'and', 'one', 'of', 'anthony', \"quinn's\", 'best', 'performance', 'ever.', 'i', 'just', 'wish', 'that', 'the', 'tv-channels', 'would', 'send', 'this', 'movie', 'more', 'often.', 'it', 'has', 'inspired', 'me', 'in', 'a', 'good', 'way', 'and', 'surly', 'will', 'do', 'that', 'to', 'many', 'others.', 'if', 'there', 'is', 'any', 'actor', 'i', 'wanted', 'to', 'meet,', 'it', 'would', 'have', 'been', 'anthony', 'quinn.', 'there', 'will', 'never', 'be', 'a', 'like', 'of', 'him', 'on', 'the', 'silver', 'screens,,,', 'ever.', 'wish', 'that', 'anthony', 'quinn', 'was', 'still', 'alive.', 'may', 'god', 'bless', 'his', 'soul.'], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# trainset 내용물 확인\n",
    "print(vars(testset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-3. Vocabulary set 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trainset, min_freq=5)      # 단어 집합 생성, 단어 수가 적으므로 최소 횟수를 5로 설정\n",
    "LABEL.build_vocab(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 847\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(TEXT.vocab)\n",
    "print('단어 집합의 크기 : {}'.format(n_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x000001CE2FE25FC8>>, {'<unk>': 0, '<pad>': 1, 'the': 2, 'and': 3, 'a': 4, 'of': 5, 'to': 6, 'is': 7, 'in': 8, 'i': 9, 'that': 10, 'this': 11, 'it': 12, 'for': 13, 'as': 14, 'with': 15, 'was': 16, '/><br': 17, 'but': 18, 'his': 19, 'not': 20, 'on': 21, 'he': 22, 'film': 23, 'movie': 24, 'by': 25, 'have': 26, 'are': 27, 'be': 28, 'you': 29, 'an': 30, 'one': 31, 'who': 32, 'at': 33, 'so': 34, 'all': 35, 'about': 36, 'from': 37, 'they': 38, 'has': 39, 'her': 40, 'like': 41, 'just': 42, \"it's\": 43, 'good': 44, 'if': 45, 'out': 46, 'more': 47, 'my': 48, 'had': 49, 'or': 50, 'she': 51, 'some': 52, 'really': 53, 'very': 54, 'when': 55, 'there': 56, 'were': 57, 'their': 58, 'would': 59, 'other': 60, 'only': 61, 'we': 62, 'what': 63, 'can': 64, 'even': 65, 'get': 66, 'no': 67, 'up': 68, 'which': 69, 'him': 70, 'into': 71, 'much': 72, 'see': 73, 'been': 74, 'after': 75, 'than': 76, 'how': 77, 'many': 78, 'story': 79, 'does': 80, 'two': 81, 'best': 82, 'first': 83, 'will': 84, '-': 85, 'do': 86, \"don't\": 87, 'its': 88, 'then': 89, 'most': 90, 'also': 91, \"doesn't\": 92, 'guy': 93, 'me': 94, 'time': 95, 'acting': 96, 'it.': 97, 'great': 98, 'any': 99, 'bad': 100, 'could': 101, 'ever': 102, 'well': 103, 'never': 104, 'because': 105, 'made': 106, 'make': 107, 'did': 108, 'love': 109, 'something': 110, '/>the': 111, 'being': 112, 'character': 113, 'plot': 114, 'way': 115, '<br': 116, 'movie.': 117, 'them': 118, 'your': 119, 'end': 120, 'little': 121, 'people': 122, 'scene': 123, 'watch': 124, 'where': 125, 'know': 126, 'over': 127, 'still': 128, 'those': 129, 'watching': 130, 'going': 131, 'better': 132, 'films': 133, 'look': 134, 'movies': 135, 'think': 136, 'years': 137, 'characters': 138, 'life': 139, 'man': 140, 'new': 141, 'off': 142, 'pretty': 143, 'why': 144, 'another': 145, 'old': 146, 'these': 147, 'too': 148, '&': 149, 'gets': 150, \"he's\": 151, 'our': 152, 'same': 153, 'seen': 154, 'show': 155, 'goes': 156, \"i'm\": 157, 'night': 158, 'should': 159, 'while': 160, 'almost': 161, 'few': 162, 'find': 163, 'part': 164, 'take': 165, 'thing': 166, 'us': 167, 'want': 168, 'go': 169, 'got': 170, 'such': 171, 'through': 172, 'around': 173, 'back': 174, 'cast': 175, 'everyone': 176, 'family': 177, 'give': 178, 'makes': 179, 'now': 180, 'between': 181, 'completely': 182, 'every': 183, \"i've\": 184, 'movie,': 185, 'seems': 186, 'someone': 187, \"that's\": 188, 'three': 189, 'tv': 190, 'come': 191, \"didn't\": 192, 'each': 193, 'enough': 194, 'found': 195, \"isn't\": 196, 'nothing': 197, 'point': 198, 'real': 199, 'though': 200, '\"the': 201, 'believe': 202, 'both': 203, 'far': 204, 'lot': 205, 'saw': 206, 'actors': 207, 'actually': 208, 'although': 209, 'beautiful': 210, 'down': 211, 'film.': 212, 'horror': 213, 'may': 214, 'own': 215, 'series': 216, 'thought': 217, 'try': 218, 'whole': 219, 'woman': 220, 'yet': 221, 'â\\x96': 222, 'american': 223, 'before': 224, 'blood': 225, 'during': 226, 'especially': 227, 'fact': 228, 'film,': 229, 'here': 230, 'however,': 231, 'need': 232, 'put': 233, 'rather': 234, \"there's\": 235, 'true': 236, 'use': 237, 'war': 238, 'action': 239, 'anyone': 240, 'big': 241, 'feel': 242, 'having': 243, 'long': 244, 'might': 245, 'must': 246, 'quite': 247, 'right': 248, 'work': 249, 'world': 250, '/>i': 251, 'again': 252, 'am': 253, 'audience': 254, 'black': 255, \"can't\": 256, 'comes': 257, \"couldn't\": 258, 'different': 259, 'girl': 260, 'hedge': 261, 'men': 262, 'minutes': 263, 'once': 264, 'probably': 265, 'scenes': 266, 'script': 267, 'takes': 268, 'young': 269, 'entire': 270, 'episode': 271, 'himself': 272, 'interesting': 273, 'it,': 274, 'last': 275, 'making': 276, 'next': 277, 'say': 278, 'shows': 279, 'time.': 280, 'without': 281, 'worst': 282, 'worth': 283, 'against': 284, 'came': 285, 'couple': 286, 'dead': 287, 'ending': 288, 'given': 289, 'lives': 290, 'me,': 291, 'music': 292, 'play': 293, 'since': 294, 'stop': 295, 'things': 296, 'western': 297, \"you're\": 298, 'able': 299, 'always': 300, 'become': 301, 'course': 302, 'director': 303, 'everything': 304, 'getting': 305, 'hollywood': 306, 'job': 307, 'less': 308, 'let': 309, 'lines': 310, 'original': 311, 'place': 312, 'played': 313, 'role': 314, 'set': 315, 'sex': 316, 'sure': 317, 'tell': 318, 'year': 319, 'act': 320, 'behind': 321, 'bruno': 322, 'death': 323, 'female': 324, 'friend': 325, 'help': 326, 'instead': 327, 'leave': 328, 'looking': 329, 'main': 330, 'marine': 331, 'moments': 332, 'obviously': 333, 'plays': 334, 'richard': 335, 'said': 336, 'screen': 337, 'seeing': 338, 'small': 339, 'special': 340, 'together': 341, 'totally': 342, 'trying': 343, 'used': 344, 'went': 345, 'beyond': 346, 'bit': 347, 'book': 348, 'certainly': 349, 'children': 350, 'crime': 351, 'erroll': 352, 'friends': 353, 'her.': 354, 'looks': 355, 'loved': 356, 'meet': 357, 'particular': 358, 'performance': 359, 'playing': 360, 'read': 361, 'rest': 362, 'run': 363, 'school': 364, 'score': 365, 'spirit': 366, 'start': 367, 'starts': 368, 'turn': 369, 'well,': 370, '(and': 371, '--': 372, 'along': 373, 'andy': 374, 'attempt': 375, 'away': 376, 'becomes': 377, 'brother': 378, 'called': 379, 'care': 380, 'cold': 381, 'dance': 382, 'dvd': 383, 'early': 384, 'excellent': 385, 'extremely': 386, 'full': 387, 'funny': 388, 'good.': 389, 'hard': 390, 'heart': 391, 'him.': 392, 'home': 393, 'hope': 394, 'imagine': 395, 'known': 396, 'knows': 397, 'least': 398, 'one.': 399, 'realize': 400, 'really,': 401, 'save': 402, 'second': 403, 'seemed': 404, \"she's\": 405, 'so,': 406, 'son': 407, 'soon': 408, 'sort': 409, 'star': 410, 'strong': 411, 'supposed': 412, \"they're\": 413, 'this.': 414, 'time,': 415, 'top': 416, 'upon': 417, 'watched': 418, 'well.': 419, \"who's\": 420, 'works': 421, '/>this': 422, '10': 423, '2': 424, 'anything': 425, 'appears': 426, 'atmosphere': 427, 'beauty': 428, 'car': 429, 'crazy': 430, 'dialog': 431, 'done': 432, 'doubt': 433, 'dull': 434, 'experience': 435, 'eyes': 436, 'fan': 437, 'features': 438, 'felt': 439, 'films,': 440, 'films.': 441, 'fine': 442, 'fun': 443, 'gore': 444, 'hell': 445, 'hero': 446, 'including': 447, 'incredibly': 448, 'is,': 449, 'lack': 450, 'local': 451, 'lost': 452, 'maybe': 453, 'mean': 454, 'meets': 455, 'moment': 456, 'money': 457, 'more.': 458, 'mother': 459, 'moving': 460, 'name': 461, 'ok': 462, 'overall': 463, 'performances': 464, 'perhaps': 465, 'piece': 466, 'police': 467, 'recommend': 468, 'shot': 469, 'single': 470, 'stars': 471, 'subject': 472, 'thinking': 473, 'tries': 474, 'truly': 475, 'until': 476, 'us.': 477, 'wanted': 478, \"wasn't\": 479, 'waste': 480, 'wife': 481, 'women': 482, 'yes,': 483, 'york': 484, '(a': 485, '(as': 486, '(the': 487, '/>as': 488, '/>in': 489, '/>to': 490, '15': 491, 'across': 492, 'again.': 493, 'already': 494, \"aren't\": 495, 'became': 496, 'body': 497, 'british': 498, 'camera': 499, 'cannot': 500, 'classic': 501, 'comments': 502, 'constantly': 503, 'days': 504, 'deniro': 505, 'direction': 506, 'effects': 507, 'else': 508, 'enjoy': 509, 'equally': 510, 'exactly': 511, 'expect': 512, 'fairly': 513, 'feeling': 514, 'good,': 515, 'guy,': 516, 'guys': 517, 'hear': 518, 'heard': 519, 'huge': 520, 'indeed,': 521, 'itself': 522, 'keep': 523, 'kind': 524, 'later': 525, 'leaves': 526, 'life.': 527, 'live': 528, 'living': 529, 'longer': 530, 'major': 531, 'me.': 532, 'message': 533, 'mostly': 534, 'mystery': 535, 'near': 536, 'nearly': 537, 'needed': 538, 'nice': 539, 'oscar': 540, 'pay': 541, 'picture': 542, 'please': 543, 'poor': 544, 'problem': 545, 'problems': 546, 'production': 547, 'pure': 548, 'question': 549, 'racism': 550, 'reason': 551, 'rock': 552, 'russian': 553, 'sad': 554, 'sampredo': 555, 'sandra': 556, 'showing': 557, 'shown': 558, 'slow': 559, 'somehow': 560, 'sometimes': 561, 'songs': 562, 'soundtrack': 563, 'stupid': 564, 'talent': 565, 'talking': 566, 'this,': 567, 'took': 568, 'town': 569, 'turns': 570, 'understand': 571, 'unfortunately': 572, 'version': 573, 'was,': 574, 'white': 575, 'whom': 576, 'whose': 577, 'wonderful': 578, 'writing': 579, 'written': 580, \"you'll\": 581, 'zombie': 582, '\"be': 583, '...': 584, '/>and': 585, '40': 586, '40-year-old': 587, 'actress': 588, 'alone': 589, 'attempts': 590, 'beginning': 591, 'bizarre': 592, 'boring': 593, 'boy': 594, 'boys': 595, 'brilliant': 596, 'broken': 597, 'chance': 598, 'change': 599, 'comet': 600, 'course,': 601, 'cut': 602, 'cute': 603, 'daughter': 604, 'david': 605, 'deal': 606, 'despite': 607, 'development': 608, 'dialogue': 609, 'doing': 610, 'easy': 611, 'end,': 612, 'enjoyed': 613, 'evil': 614, 'face': 615, 'father': 616, 'favorite': 617, 'feelings': 618, 'fell': 619, 'final': 620, 'flick': 621, 'focus': 622, 'following': 623, 'gas': 624, 'general': 625, 'guess': 626, 'harris': 627, 'harry': 628, 'here,': 629, 'here.': 630, 'high': 631, 'horrible': 632, 'images': 633, 'important': 634, 'involving': 635, 'john': 636, 'julia': 637, 'killing': 638, 'knowing': 639, 'lead': 640, 'leading': 641, 'left': 642, 'macmurray': 643, 'male': 644, 'manages': 645, 'mary': 646, 'merely': 647, 'mind': 648, 'missing': 649, 'move': 650, 'murder': 651, 'opening': 652, 'others': 653, 'particularly': 654, 'past': 655, 'perfect': 656, 'period': 657, 'plot.': 658, 'possibly': 659, 'relationship': 660, 'remake': 661, 'rent': 662, 'sarne': 663, 'saving': 664, 'scientist': 665, 'sense': 666, 'sequence': 667, 'sexual': 668, 'side': 669, 'spend': 670, 'stuff': 671, 'that.': 672, 'them.': 673, 'themselves': 674, 'times': 675, 'tony': 676, 'towards': 677, 'turned': 678, 'under': 679, 'usual': 680, 'valjean': 681, 'vegas': 682, 'video': 683, 'virgin': 684, 'ways': 685, 'working': 686, 'worthy': 687, ',': 688, '/>they': 689, 'above': 690, 'absolutely': 691, 'actor': 692, 'actors,': 693, 'air': 694, 'alan': 695, 'among': 696, 'andrews': 697, 'angela': 698, 'ann': 699, 'appear': 700, 'appearance': 701, 'awful': 702, 'baby': 703, 'bad,': 704, 'bill': 705, 'bobby': 706, 'breaks': 707, 'buddies': 708, 'building': 709, 'calls': 710, 'certain': 711, 'check': 712, 'cheesy': 713, 'chris': 714, 'cinema': 715, 'cinematic': 716, 'cinematography': 717, 'comedy,': 718, 'complete': 719, 'copy': 720, 'cosette': 721, 'created': 722, 'decide': 723, 'definitely': 724, 'die': 725, 'easily': 726, 'ending,': 727, 'english': 728, 'ethical': 729, 'fact,': 730, 'fails': 731, 'famous': 732, 'fans': 733, 'fellow': 734, \"film's\": 735, 'finally': 736, 'five': 737, 'forced': 738, 'fun.': 739, 'funny.': 740, 'gave': 741, 'girls': 742, 'god': 743, 'greatest': 744, 'guy.': 745, 'hair': 746, 'halloween': 747, 'happens': 748, 'historical': 749, 'hold': 750, 'hot': 751, 'however': 752, 'human': 753, \"i'll\": 754, 'idea': 755, 'impressive': 756, 'includes': 757, 'independent': 758, 'instead.': 759, 'involved': 760, 'japanese': 761, 'journey': 762, 'kills': 763, 'ladies': 764, 'lame': 765, 'las': 766, 'late': 767, 'laugh': 768, 'leon': 769, 'liked': 770, 'line': 771, 'low': 772, 'made.': 773, 'manner': 774, 'me\"': 775, 'means': 776, 'meant': 777, 'movies.': 778, 'none': 779, 'of,': 780, 'oh,': 781, 'on,': 782, 'ones': 783, 'opinion': 784, 'outside': 785, 'people,': 786, 'perfectly': 787, 'person': 788, 'plain': 789, 'plots': 790, 'points': 791, 'portrayed': 792, 'power': 793, 'present': 794, 'protagonist': 795, 'quickly': 796, 'rarely': 797, 'red': 798, 'release': 799, 'remember': 800, 'responsible': 801, 'ridiculous': 802, 'robert': 803, 'role.': 804, 'sam': 805, 'seem': 806, 'seen.': 807, 'sent': 808, 'sequel': 809, 'sequences': 810, 'several': 811, 'shots': 812, 'show,': 813, 'simple': 814, 'simply': 815, 'singing': 816, 'somewhat': 817, 'song': 818, 'stands': 819, 'started': 820, 'storyline': 821, 'supporting': 822, 'surprisingly': 823, 'taking': 824, 'television': 825, 'tells': 826, 'terrible': 827, 'terribly': 828, 'theater': 829, 'thinks': 830, 'throughout': 831, 'token': 832, 'told': 833, 'total': 834, 'turning': 835, 'typical': 836, 'unrealistic': 837, 'up,': 838, 'uses': 839, 'usually': 840, 'viewer': 841, 'viewing': 842, 'walking': 843, 'weird': 844, 'worse': 845, 'wrong': 846})\n"
     ]
    }
   ],
   "source": [
    "# 수집된 전체 단어 확인\n",
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_2, valset = trainset.split(split_ratio=0.75)       # 훈련 데이터와 평가 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 개수 : 112\n",
      "평가 샘플의 개수 : 38\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플의 개수 : {}'.format(len(trainset_2)))\n",
    "print('평가 샘플의 개수 : {}'.format(len(valset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-4. Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "        (trainset_2, valset, testset), batch_size=BATCH_SIZE,\n",
    "        shuffle=True, repeat=False, sort=False)                # shuffle 진행 안하면 Data 변환 추적 쉬움."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치의 개수 : 28\n",
      "검증 데이터의 미니 배치의 개수 : 10\n",
      "테스트 데이터의 미니 배치의 개수 : 13\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 미니 배치의 개수 : {}'.format(len(train_iter)))\n",
    "print('검증 데이터의 미니 배치의 개수 : {}'.format(len(val_iter)))\n",
    "print('테스트 데이터의 미니 배치의 개수 : {}'.format(len(test_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x727]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))       # Dataloader가 iterator 역할을 잘하는지 확인.\n",
    "print(batch)                         # 재실행 때마다 sample 크기 변해야 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x303]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n"
     ]
    }
   ],
   "source": [
    "batch2 = next(iter(val_iter))      # data.BucketIterator.splits 조건에 sort=False를 하지 않으면\n",
    "print(batch2)                      # \" '<' not supported ... \" Error 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 샘플의 개수 재확인 : 112\n",
      "검증 데이터의 샘플의 개수 재확인 : 38\n",
      "테스트 데이터의 샘플의 개수 재확인 : 50\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 샘플의 개수 재확인 : {}'.format(len(train_iter.dataset)))\n",
    "print('검증 데이터의 샘플의 개수 재확인 : {}'.format(len(val_iter.dataset)))\n",
    "print('테스트 데이터의 샘플의 개수 재확인 : {}'.format(len(test_iter.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch Iterator test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b is 0. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x727]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 1. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x163]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 2. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x233]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 3. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x496]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 4. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x421]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 5. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x305]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 6. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x460]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 7. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x232]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 8. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x148]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 9. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x866]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 10. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x756]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 11. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x523]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 12. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x191]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 13. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x207]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 14. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x612]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 15. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x353]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 16. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x440]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 17. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x267]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 18. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x761]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 19. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x315]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 20. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x318]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 21. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x810]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 22. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x545]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 23. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x204]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 24. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x427]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 25. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x654]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 26. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x785]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n",
      "b is 27. batch is \n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x360]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n"
     ]
    }
   ],
   "source": [
    "for b, batch in enumerate(train_iter):\n",
    "    x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "    print('b is {}. batch is {}'. format(b, batch))              # batch 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is tensor([[  9, 800,   0,  ...,   1,   1,   1],\n",
      "        [  0,   0,  15,  ...,   1,   1,   1],\n",
      "        [  9,   0,  11,  ...,  16, 475,   0],\n",
      "        [  9,   0,  11,  ...,   1,   1,   1]]). y is tensor([1, 2, 2, 1])\n",
      "tensor([0, 1, 1, 0])\n",
      "x is tensor([[370,  48,   0,  ...,   1,   1,   1],\n",
      "        [  0,   0,  77,  ..., 704, 102, 493],\n",
      "        [ 31,   5,   2,  ...,   1,   1,   1],\n",
      "        [ 11,  24,   7,  ...,   1,   1,   1]]). y is tensor([1, 2, 1, 2])\n",
      "tensor([0, 1, 0, 1])\n",
      "x is tensor([[  9,  49,  74, 329,   0,   6, 338,   0,  13, 247,   4,   0,  15,  35,\n",
      "          43,   0,   0,   0,   3,   0,   0,   3,   9, 246,   0,   2,  83,   0,\n",
      "           5,   2,  24,  16,   0,  12,  53, 792,   2, 255, 292, 123, 174,   0,\n",
      "         231,  14,   2,  24,   0, 782,  94,   3,  48, 219, 177,  57,   0,  46,\n",
      "           5, 152,   0,   2, 816,  42,   0,   0,  31,  75,   2,   0,   9, 454,\n",
      "           0,  42,  31,  47, 292,   0,   3,  12,  59,  26,   0,  65,  15,   0,\n",
      "          17,   0,   9,   0,   0,  99, 113, 608,   8,  99,   5,   2,   0,   9,\n",
      "          42, 192, 380,  63,   0,   6,   0,  65,  55,   0,   0, 113,   0,   5,\n",
      "           4,   0,   0,   9,   0,   9, 159,  26,  74,   0,  18,   9,  42, 258,\n",
      "         242,  99,   0,  13,  10,   0,   2, 138,  57, 289,   4,   0,   0,  36,\n",
      "         816,   8,  58,   0,   3,   0,  18,  56,   0,  57,  20,   0, 194,   6,\n",
      "           0,  94,   0,  17,   0,   2,   0,  16, 815,  20,   0, 194,   6, 107,\n",
      "           2, 841,   0,  69, 156, 373,  15,   2, 450,   5, 113,   0,  11,  24,\n",
      "           0,  94,   5,   4,   0,  24,   0,  21,   0,   0,   3, 662,   0,   3,\n",
      "           0,  57, 578, 135,   8,  48,   0, 463,   9, 136,  11,  24,  59,  82,\n",
      "           0, 187,  32,  92,  53, 380,  36,  30, 463,   0, 221,  59, 509,  81,\n",
      "           0,   5,   0,   3, 443, 816,   0],\n",
      "        [  4,  54, 388,   0,  23,   0,  25,   2,   0,   5,   0,   0,   0,   0,\n",
      "           8,   2,   0,   3,   2,   0,   3,   0,   5, 761,   0,   8,   2, 477,\n",
      "         315,   8, 297,   0,  12, 438,  98, 464,  25,   0,   0,   0,   0,   3,\n",
      "           0,   0, 292,  25,   0,   0,   0,   0,   0,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1],\n",
      "        [ 41,  60, 122,  32,   0,  21,   0,   0,   9,   0,  25, 598, 417,  11,\n",
      "         121,   0,  21,   0, 190, 281, 243, 519,   5,  12,   0,   2,   0,   0,\n",
      "           5,   4,   0,   0,  79,  36,   4,   0,  18,   0,   0,  21,   2,  31,\n",
      "           0,   3,   4,   0,   3,   0,   0,   5, 238,   8,   2,   0,  21,   2,\n",
      "          60,   0,  49,  94,   0,  37,   2,   0,  17, 490,  94,  11,   7,  31,\n",
      "           5,   2, 737,  82, 135,  36,   0, 487, 653,  27,   0,   0,  21,   2,\n",
      "         297,   0,   0,   5,   0,   0,   3,   2,   0,   0,   0,   0,   0,   0,\n",
      "           3,   2, 123,  15,   2,   0, 624,   0,   7,  53,   0,   2,   0,   3,\n",
      "         262, 700,  41,   0,   5,   2,   0,  15,  58, 624,   0,  17, 251,  61,\n",
      "           0,   9,  49,   0,   2, 212,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1],\n",
      "        [  9,  42, 418,  11,  23,  11,   0,   3,   9, 195,  12,   6,  28,   4,\n",
      "          98, 557,   5,   2,   0,   5,   0,   0, 741, 118, 145, 115,   6, 134,\n",
      "          33,   0,  20,   4,   0,  18,  30,   0,  51,   0,  35,  10,  51,  49,\n",
      "          15, 129,  32, 741,  63, 121,  38,  49,   6, 354,   9,  73,   2,  79,\n",
      "           5, 743,   8, 630,  22, 808,  19,  61, 407,   6,   0, 140, 101,  20,\n",
      "         659, 178, 425,  10,  59,   0, 672, 406,  13, 152, 339,   0,  62,  27,\n",
      "         289,  30,   0,   0,   3,  27,   0, 105,   5,  97,   8,  11,  23,   2,\n",
      "           0,   0,  26,  34,   0, 674,  15,   4, 339,   0,   5,   0,   0,   0,\n",
      "         118,  10, 139,   3, 743,  64,   0,  28, 210,   8,  43,   0,   0,   2,\n",
      "         109,  10,   0, 407,   0,   6, 140,   7,   2, 109,  62, 159, 155,   6,\n",
      "          31, 145,   3, 152, 290,  84,  28,   2,   0,  13,  97,  65,   2,  23,\n",
      "           7,   4,   0,  12, 186, 559,   8,   2,   0,  18,   2,   0,   5,  95,\n",
      "           3,   0,   6,   0,   7,   0,   8,   2,   0,  12,  16, 475,   4,   0,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1]]). y is tensor([2, 1, 1, 1])\n",
      "tensor([1, 0, 0, 0])\n",
      "x is tensor([[ 45,  29, 285, 629,  43, 105,   0, 494, 154,  11,  23,   3,  57,   0,\n",
      "          63, 653,  49,   6, 278,  36,   0,  17, 251, 242,  13,   0,   9,   0,\n",
      "           0,   3,   9,   0,   0,  14,   4,   0,   0, 188,  63,  62,   0,  10,\n",
      "          11,  23, 102,  49,   6,   0, 119,   0,  45,  61,  13,   4,   0,   9,\n",
      "         518,  56,   7,  67,   0,  13,   2,   0,   0,   0,  46,   5, 183,   0,\n",
      "           0,  17,   0,  41, 176,   0,   0,  11,  24,   0,  12,   6,  28,  52,\n",
      "         564,   0,   0,   5,   0,   9,   0,   0,   9, 101, 237,   4,  44, 768,\n",
      "          33,   4, 564,   0, 754, 178,   2,   0,   5,  11,  23,  31,   0,   0,\n",
      "          36,  58,   0,   0,  13,   0,   2,   0,   0,  37,   2,   0,   0,   0,\n",
      "           5,   2,   0,   9,  64,  73,  77,  10,  59,  26,  74,   4,   0,  37,\n",
      "         338,  11,   0,  17, 585,  13,   2,   0,   9,   0,   2, 185,  18, 108,\n",
      "          20, 124,  97,   2, 609,  16,   0,   3,  90,   5,   2, 266, 568, 312,\n",
      "           8,  48,   0,   0,   9,   0,   6,   0,  17,   0, 126,   0, 157,  20,\n",
      "         579,   0,  36, 414,  43,  42, 148,   0],\n",
      "        [  6,  35,   2,   0,  46,   0,   0,   4, 190, 216,  15,  31, 271,   7,\n",
      "          41,   0,  30, 270, 348,  75,   0,   2,  83, 162,   0,  10, 112,   0,\n",
      "           9,  16,   4, 520, 437,   5,   0, 217,  12,  16,  52,   5,   2,  82,\n",
      "         190,   0,   0,  42,   0,  21,   2,   0,  18,  12,  16,  95,  13,  12,\n",
      "           6, 120,   3,   2,  79,   6, 650,   0,   9, 770,   2,   0,   0, 201,\n",
      "           0,  18,  12,   0,  14,  78, 141,   0,  14,  12,   0,  34,   9,   0,\n",
      "           0,   0,   3,   9,  16,  20,   0,  17, 251,  16,   0,  12,  59,  20,\n",
      "         218,   6,  28, 145,   0,   3,   9,  16,   0,  10,   0,   7, 110,   0,\n",
      "           3,   9,   0, 195,   2,  79,   0,   8,   4, 259, 115,  76,   0,   0,\n",
      "           0,   0,   3,   0, 340, 507,   0,  66,  94,   0,   9,  41,  10, 671,\n",
      "           0,   0,   0,   3,   0,   0, 178,   0,   0,   3,   0,  16,  42,   0,\n",
      "           9, 256,   0, 476, 277,   0,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [  4,   0,   5,   0, 742, 107,   4, 141,   0, 670,   2, 158,   8,   4,\n",
      "           0,   0,   5, 302,   2,   0,   0,  87,   0,   0,  13, 244,   3,  35,\n",
      "         445, 707,   0,   0,   0,   7,  30,   0,   0, 213,  15,  52,   0, 287,\n",
      "           0,   0, 173,  10,  27, 112,   0,  25,   2, 366,   5,   4, 287,   0,\n",
      "           0,   7,  67, 444,   3,   0,   2, 427,   5,   4,   0,   7,  54,   0,\n",
      "          96,   7,   0,   2, 267, 268, 148,  72,  95,   6,   0,   2, 138, 476,\n",
      "           2, 620,   0, 263,  10,   0, 736, 707,  46,   5,  19,   0, 717,   7,\n",
      "         756,   3,   2,   2,   0,   7,   4,  98,   0,  13,   2,   0,   0,  23,\n",
      "         268,  34, 244,   6,  66, 131,   3,  11,   7,  88, 531,   0,  46,   5,\n",
      "           0,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [ 11,  23,  39,   4,   0,   0,  69,   7, 247,   0,   6,   2,   0,   0,\n",
      "           0,   0,   5,   2,   0,   3,  60,   0,   0,   0,   0,  64,  28,   0,\n",
      "           0,  14,  38, 237,   0, 421,  14,  58,   0,   0,   0,   0,   0, 821,\n",
      "           0,   2,  54,   0,   5, 753,   0, 227,  10,   5,   0,   0, 607,   2,\n",
      "         228,   2,  23,  16,   0,  21,   4,   0,   0,  43, 208,   4,   0,   5,\n",
      "           2,   0,   8,   0,   0,   2, 382, 810,  27,  20,   0, 221,   2, 562,\n",
      "           0,  77,   0,   0,   0,   9,   0,   2,   0, 159,   0,   0, 115,   5,\n",
      "           0,  25,  20,   0,   2, 228,  10,   0, 382, 608,   0,   6,  28,  47,\n",
      "           0,   2, 382, 810, 806,   0,   8,  52, 685,   3,  20,   0,  15,   2,\n",
      "         562,   0,   0,   0,   9, 128, 109,  11,  24,   3,   0,  12,  14,   4,\n",
      "         141,   0,   5,   0,  23,   0,  69,   9, 394,   6,   0,   8,   2,   0,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1]]). y is tensor([2, 1, 1, 1])\n",
      "tensor([1, 0, 0, 0])\n",
      "x is tensor([[  9,  64,   0,  ...,   1,   1,   1],\n",
      "        [802,  23, 125,  ...,   1,   1,   1],\n",
      "        [  0,   0,   6,  ...,   1,   1,   1],\n",
      "        [  9,  49, 361,  ...,   2, 319,   0]]). y is tensor([1, 2, 1, 2])\n",
      "tensor([0, 1, 0, 1])\n",
      "x is tensor([[ 28,   0,  11,  ...,   1,   1,   1],\n",
      "        [ 11,  24,  16,  ...,   1,   1,   1],\n",
      "        [ 75,  52,   0,  ...,  27, 834,   0],\n",
      "        [  9,  83, 285,  ...,   1,   1,   1]]). y is tensor([2, 2, 2, 1])\n",
      "tensor([1, 1, 1, 0])\n",
      "x is tensor([[  9, 109,  11,   0,   0,   0, 285,  46,   2, 319,   9,  16,   0,   3,\n",
      "          12,  39,  49,  48, 391, 294,   9,  64,   0,   0,   7,  34,   0,   8,\n",
      "          11, 117,   9, 345,   6,   4, 340, 557,   5,   0,   0, 275, 158,   3,\n",
      "          12,  16,  41,   4,   0,   9,  16,   0,   6,  73,  52, 236, 733,   0,\n",
      "          11,  24,   7,  34,   0,  12,   7,  53,  31,   5,   2, 744, 135,   5,\n",
      "          35, 280,   2, 292,   7,   0,   2,  24,   7,  36, 201,   0, 313,  25,\n",
      "           0,  19, 177,   7,   0,  19,   0,   7,   2,   0, 320,   8,   0,   3,\n",
      "          22,  39,  19, 436,  21,   2,   0,  30,   0,   0,  56,   7,  67, 549,\n",
      "         144,   0,   7,  48, 617,   0,   9,  64,   0, 201,   0,  13, 672,  34,\n",
      "          45,  29,  26,  20, 154,  11,  89,  29,  27, 232,   6,   0,  11,   7,\n",
      "           4, 501,  85,   0,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1],\n",
      "        [  4,  54, 273,   0,   6,   2,   0,   0,   0,   0, 842,  13, 129,  32,\n",
      "          41,  58, 135,   0,  48,   0,   5,   2,   0, 288,  59,  28,  10,   0,\n",
      "         762,   0,   8,   0,  22,   0,   2,   0,   0,   0,   5,   0,   4, 409,\n",
      "           5,   0,  18,  20,   0,   0,  20, 305,   0,  85,  65,   2,   0,   0,\n",
      "           0,  87, 295,  33,   2,   0,  12,  39,   4,   0,   0,  41,  29, 159,\n",
      "          28, 299,   6,   0,   0,   0,  18,  29,  64, 104, 208,   0, 129,   0,\n",
      "          22,   7,   0,   6,   0,   0,   0,   0,   3,   0,  68,   0,  19,   0,\n",
      "         762,   6,  88, 620,   0,   2, 146,   0, 573,   5, 445,   0,  20, 445,\n",
      "           2,   0,   0, 196,   0, 751,  41,   2, 498,   0,  88,   0,   0,   2,\n",
      "           0,   5,   0,   0,   5, 381,   3,   0,  11, 426,   6,  28, 125,   0,\n",
      "         120, 838, 243,  42,   0,   6,   0,   0, 172,  10,   0,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1],\n",
      "        [  0,   0,  42, 145, 621,  15,   0,   0,   0,   6,  28,  52,   0,   0,\n",
      "          18,   0,   0,   3, 638,  41,  12,  16,   0,   0,   5, 601,   0,  12,\n",
      "           8,   2, 120,  25,   0,   2, 457,   6,  30,   0,  77,   0,   0,   2,\n",
      "         250,   6, 480,   3,   0,   0,  12,  45,  29, 165, 380,   5,   4, 121,\n",
      "           0,   0,  11, 415,  22,   7,   0,  15,   0,   0,   0,   0,   0,   0,\n",
      "          26, 187,   6,   0,  35, 129,   0, 310, 782,   0,   3,  56,  27,   0,\n",
      "           2, 431,   7, 548,   0,  17, 689, 169,  75, 100, 517,   0,   0,   3,\n",
      "           0,   0,  81, 122,  10,  57,   0,   6, 293,   2,   0,  17,   0,   0,\n",
      "           0,   0,   3,   0,   0, 410,   0,   0,  77,   0,  17, 251, 626,  45,\n",
      "          29,   0,  26,  52,   0,   0,   0,  11, 214,   0,  45, 197, 508,  12,\n",
      "          84, 326,  29,  66, 119,   0,   8,   0,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1],\n",
      "        [691,  31,   5,   2, 282, 135, 184, 154,   8,   4, 244,   0,  12, 368,\n",
      "         142,   0,   3,  42,   0,   0,   0,   7,   0,   0,   8,   4,   0, 314,\n",
      "           3,   0,   0,   0,   0,  15,  63,   7,   0,   4,   0,   0,   0,   2,\n",
      "          61,   0,  15,  99,   0,   7,   0,   0,  32,   0,  12,  68,  14,   4,\n",
      "           0,   3, 156, 182,   0,  15,  19, 804,   2, 267,   7,   0,   2,  96,\n",
      "         632,   3,  12,  39, 114,   0, 241, 194,   6,   0,   4,   0,   0,   0,\n",
      "          12,   7,  91,   2,  90,   0,  24,   9,  26, 102,   0,   0,   0, 113,\n",
      "           7, 182,   0, 405, 154,  14,  30,   0,   0,   0,  32,   0,   2,   0,\n",
      "           0,   0, 140,   6,   0,  21,  19,   0,  12,   7,   0, 831,   2,  24,\n",
      "          10, 405,   0,   3,   2, 533,  10,   0,  10,   0,  42,   0,   0,  33,\n",
      "           2, 612,  51, 196,  65, 299,   6,   0,   0,  25,   0,   2, 140, 420,\n",
      "         333,   0, 301,   0,  15,   0,   3,   0,  18,   2, 267,   0,  70,   6,\n",
      "           0,   0,   0,   0, 272,   8,   2, 436,   5,   0,   0,   0,  87, 480,\n",
      "         119, 280]]). y is tensor([1, 1, 2, 2])\n",
      "tensor([0, 0, 1, 1])\n",
      "x is tensor([[ 65, 491, 137,  75,   2, 120,   5,   2,   0, 238,   0, 285,  20, 148,\n",
      "         767,  50,  16,  65,   0,  43,  31,   5,   2, 162,  10, 218,   6, 606,\n",
      "          15,   2, 403, 554, 669,   5,   2,   0,   2,  95,   0, 259,  37, 135,\n",
      "          41,   0,   0,  50,   0,  69, 237,   6, 794,  58, 330, 138,  14, 597,\n",
      "           0,   8,   4, 100,  75, 238,   0,  11,  24,   0,   2, 254,   6, 615,\n",
      "           4, 259,   0,  21,   2,   0,   0,  58, 608,   7, 558,  54,   0, 224,\n",
      "           3, 227,  75,   2,   0,   2, 546,  27,   0,  18,   8,  35,  11,   0,\n",
      "          56,   7, 300,   2, 514,   5,  52, 394,  21,   2,   0,   5, 109,   3,\n",
      "           0,   0, 245,  28,   2,   0,   0,  24, 102,  18,  75, 161, 491, 137,\n",
      "          11,   7,  53,   0,   3,   0,   0,   0,  12,  64, 107, 167, 202,  10,\n",
      "           2, 238,  39,  20,   0,   0,  33, 398,  13,  52,   5,   0,  17, 111,\n",
      "         189, 330, 138,  27,   0,   0,   0,  39, 432,  31,   5,  19,  82,   0,\n",
      "          18,   0, 627,   7,   2, 410,   5,  11, 117,   0,  16,  19,  82, 359,\n",
      "           0],\n",
      "        [  0,   0,  14,   0,   0, 665,  32, 839,   0,   6,   0,   0,   8,   0,\n",
      "           6,   0, 139,   0,  13,   0,   0,   2,   0,   8,  11,  23,   7,   0,\n",
      "           4, 205,   5,   0,   0,   4, 205,   5,   0,   3,  67, 114, 179, 130,\n",
      "          11,  23,   4,   0,   9, 356,  77,  35,   2,   0, 619, 341,   8,   2,\n",
      "         120,   8, 836, 306,   0,   2,  79, 104, 150,   0,   3,  29, 242,   0,\n",
      "          14,  29,   0,  17,   0,   0, 365,   0,   0,  41,  11,  31,   0,  18,\n",
      "           2,   0,   0,  42, 194,   0,   3,   0,   6,   0,   4, 286,  47,   0,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1],\n",
      "        [  2,  44, 166,  36,  11,  23,   7,  10,  12, 819, 589,  85,  29,  87,\n",
      "          26,   6,  26, 154,   2,   0, 572,  11,   7,  91,  43,   0,   0,  12,\n",
      "          59,  26,  74, 539,   6,  26,   0,   4, 162,   5,   2, 311, 138,   8,\n",
      "           2, 141,  79,   3, 154,  77,  58, 290,  49,   0,   0,  14,   8,   2,\n",
      "         311,   7, 385,   3,   0,   2, 133,  82,   0, 332,  14,  22, 590,   6,\n",
      "         606,  15,   0,   3,   0,   0,  18,   2, 822, 175,   7,  20,  14, 411,\n",
      "          14,   8,   2, 311, 117,   0,   7,   6,  28,   0,  21,   4,   0, 375,\n",
      "           6, 650,   2, 113,  21,   3,   0,  30, 311, 809,  18,   2,  23,   7,\n",
      "           0,   0,   3,   0,   2,   0,   5,   2, 311,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1],\n",
      "        [202,  12,  50,   0,  11,  16,  33,  31,  95,   2, 282,  24,   9,  49,\n",
      "         102, 807, 294,  10, 415,   9,  26, 154,  78,  47, 135,  10,  27, 845,\n",
      "           0,   7,  12,   0,   0,   6,  28,   0,   9,  49,   6, 178,  11,  24,\n",
      "           4, 424,  46,   5,   0,  18,  12,  16,   4,   0,   0,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1]]). y is tensor([1, 2, 1, 2])\n",
      "tensor([0, 1, 0, 1])\n",
      "x is tensor([[  9,   0,  11,  ...,   1,   1,   1],\n",
      "        [  2, 190,   0,  ...,  24,   6,   0],\n",
      "        [  9,   0,   2,  ...,   1,   1,   1],\n",
      "        [201,   0,   0,  ...,   1,   1,   1]]). y is tensor([2, 2, 1, 1])\n",
      "tensor([1, 1, 0, 0])\n",
      "x is tensor([[184, 154,   0,  ...,   1,   1,   1],\n",
      "        [  0,   0,  82,  ...,   1,   1,   1],\n",
      "        [264, 252,   0,  ...,   1,   1,   1],\n",
      "        [209,   2,   0,  ...,  28,   0,   0]]). y is tensor([2, 1, 2, 2])\n",
      "tensor([1, 0, 1, 1])\n",
      "x is tensor([[ 33, 398, 696,  ...,   1,   1,   1],\n",
      "        [  9,  42, 379,  ...,   1,   1,   1],\n",
      "        [ 45,  29, 102,  ...,  13,   2,   0],\n",
      "        [  9, 356,  11,  ...,   1,   1,   1]]). y is tensor([2, 1, 1, 2])\n",
      "tensor([1, 0, 0, 1])\n",
      "x is tensor([[145,   0,   0,  ...,   1,   1,   1],\n",
      "        [  2,  83, 166,  ...,   1,   1,   1],\n",
      "        [  2,  24, 368,  ..., 695,   0,   0],\n",
      "        [ 31,   5,   2,  ...,   1,   1,   1]]). y is tensor([1, 1, 2, 1])\n",
      "tensor([0, 0, 1, 0])\n",
      "x is tensor([[  0, 126,   2,  ...,   1,   1,   1],\n",
      "        [  0,   0,   7,  ...,   1,   1,   1],\n",
      "        [  2,  79,   5,  ..., 171,  44,   0],\n",
      "        [543, 107,  94,  ...,   1,   1,   1]]). y is tensor([2, 1, 2, 2])\n",
      "tensor([1, 0, 1, 1])\n",
      "x is tensor([[  0,  36, 112,   0,  17, 251, 170,  11,   0,   4,   0,   0,  71,   2,\n",
      "         139,   5,   2, 140,  32,   0,   2,   0, 158,  21,   2,   0,   0,   9,\n",
      "           0,   6,  73, 430,   0,   3,   0,   5,  30,   0, 140,   3,   6,   0,\n",
      "          19,   0,  13, 171, 592,   0,   0,  16,   9,   0,  17,   0,   3,   0,\n",
      "           7, 534, 593,  15,  61,   0, 332,   5,   0,   0,  43, 349, 197,   6,\n",
      "           0, 393,  36,   3,   0, 536,  14,  44,  14,   0,  69,   7, 110,  10,\n",
      "           9,   0,  29, 124, 759, 328,  11,   0,   0,  45,  29, 691, 246,  73,\n",
      "          89,   0, 662,  12,  50, 163,  12,   0,   0,  86,  20, 541, 416,   0,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [ 30, 386,   0,  23,  10, 349, 196,   0,   0,  43,   0,   6,   0,   2,\n",
      "         435,   5, 130,  97,   2,   0,   0, 825,   0,  16,   0,  85, 148,   0,\n",
      "           3,   0,  11, 311, 645,   6,   0, 522,   8, 119,   0,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [ 56,   7,   4, 123, 536,   2, 591,  75,   4,   0, 125,   0,  27,   0,\n",
      "          45, 110, 798,   0, 119,   0,  12,   7, 105,   4, 575,   0,   7,   0,\n",
      "         321,   4,   0,  25,   2,   0,   9, 217,   9,  49, 154, 100,  18,  11,\n",
      "           7,  97,   4, 575,   0,   8,   4,   0, 108,  38,  20,   0,   0,   0,\n",
      "         370,   3,   9,   0, 416,   0,  33,   2,   0,  12,  84, 107,  29, 168,\n",
      "           6,   0, 119, 708,   3,  26, 118,  35, 233,   8, 423,   0,   3, 107,\n",
      "           4, 132, 117,   2, 566,  16,  34,  34,   0,   2,  96,  16, 534, 462,\n",
      "          18, 258,  28,   0,   0,   0,   6,   2, 544,   0,   5,   2,   0,  56,\n",
      "           7,   4,   0,  33,   2,   0,  10, 355,  41,   4,   0,   0,  15,   2,\n",
      "         358,   0,   9, 278, 124,  11, 185,   3, 650,   0, 594,  71,   0,  21,\n",
      "           2, 282,   5,  35, 280,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [  0, 330,   0,   5,  11,   0,  10,   0,   0, 355,   0, 417,   0,  19,\n",
      "           0,   0,   2,   0,  27,  42,   2, 121, 347,   5, 599, 538,   6,   0,\n",
      "         653, 151,   4,   0,   2, 123,   8,  69,  22,   0,   6,  19,   0,   0,\n",
      "           0,  29, 126,  63,  48,   0,   0, 373,  15, 145,   0,  15,  19,   0,\n",
      "           0,  10, 151,  49,   6,   0,  11, 153,   0,   8,   2,   0,   2,   0,\n",
      "         180, 107,  70, 134,  42,   0, 194,   6,   0,  19,   0,   3,   0,   0,\n",
      "           0,  89,  56,   7,  19, 141,   0,   0,   0,   3,   0,  13,   0,   0,\n",
      "          69,   5, 302, 776, 405,   0,   2, 219, 198,   5,   2,  23,   7,  77,\n",
      "         129, 121,   0,   0, 301,   2, 270,   0,  13,   0,   0, 116,  17,   0,\n",
      "          22,  39,   4,   0,  22, 246,  28,   0,  45,  22,  39, 244,   0,  22,\n",
      "           0,   0,  45, 151,   0, 151, 170,   4,   0,  45,  22,  39,   0, 255,\n",
      "           0,   3, 151,   0,   5,   0,  22, 246,  28,   0, 129,   0,  35,   0,\n",
      "         510,   0,   6, 532,  45,   2,   0, 122,  57,   0,   6,   8, 622,  16,\n",
      "           0,   2, 219, 198,   5,   2,  79,  59,  28,   0]]). y is tensor([2, 1, 2, 1])\n",
      "tensor([1, 0, 1, 0])\n",
      "x is tensor([[  8,   2, 504,  ...,   4, 246,   0],\n",
      "        [140,   6, 140,  ...,   1,   1,   1],\n",
      "        [  0,   0,   0,  ...,   1,   1,   1],\n",
      "        [ 87,  66,  94,  ...,   1,   1,   1]]). y is tensor([2, 2, 1, 2])\n",
      "tensor([1, 1, 0, 1])\n",
      "x is tensor([[  0,   9, 256,  ...,   1,   1,   1],\n",
      "        [453, 112,   4,  ...,  24,   0,   0],\n",
      "        [  0,   0,   8,  ...,   1,   1,   1],\n",
      "        [ 55,   9, 206,  ...,   1,   1,   1]]). y is tensor([1, 2, 1, 2])\n",
      "tensor([0, 1, 0, 1])\n",
      "x is tensor([[  4, 199,   0,  ...,   1,   1,   1],\n",
      "        [ 11,   7, 145,  ...,   1,   1,   1],\n",
      "        [  0,   0,   7,  ...,   1,   1,   1],\n",
      "        [ 11, 349,   7,  ...,   0,   0,   0]]). y is tensor([2, 1, 1, 1])\n",
      "tensor([1, 0, 0, 0])\n",
      "x is tensor([[158,   5,   2,  ...,   1,   1,   1],\n",
      "        [  0,   0,   0,  ...,   6, 165,   0],\n",
      "        [ 11,   7, 659,  ...,   1,   1,   1],\n",
      "        [  9,  42, 206,  ...,   1,   1,   1]]). y is tensor([2, 1, 1, 1])\n",
      "tensor([1, 0, 0, 0])\n",
      "x is tensor([[  2, 303,   3,  ...,   0,   0,   0],\n",
      "        [ 11, 271,   0,  ...,   1,   1,   1],\n",
      "        [  4,  24,  39,  ...,   1,   1,   1],\n",
      "        [  9,   0, 519,  ...,   1,   1,   1]]). y is tensor([1, 1, 2, 1])\n",
      "tensor([0, 0, 1, 0])\n",
      "x is tensor([[ 11, 383, 840,  ...,   1,   1,   1],\n",
      "        [202, 291,   9,  ...,   1,   1,   1],\n",
      "        [ 11,   7,   4,  ...,   1,   1,   1],\n",
      "        [  0,  78,   0,  ...,  13,   4, 212]]). y is tensor([1, 2, 2, 2])\n",
      "tensor([0, 1, 1, 1])\n",
      "x is tensor([[145, 213, 621,  ...,   1,   1,   1],\n",
      "        [ 11, 155,  49,  ...,   1,   1,   1],\n",
      "        [  2,  23,   0,  ...,   1,   1,   1],\n",
      "        [  9,  26, 344,  ...,   3, 716,   0]]). y is tensor([2, 2, 2, 1])\n",
      "tensor([1, 1, 1, 0])\n",
      "x is tensor([[  0,   0, 116,  17, 111,   0,   7, 534,  30, 239, 117,   3,  63, 239,\n",
      "          56,   7,   0,   0,   0,   0,   3,  72,  72, 458,  12, 471,   0,   0,\n",
      "           3,   0,   0,  14,   2,   0,   0,  17, 422,   7,   4, 661,   3,   9,\n",
      "          26,  20, 154,   2, 311,  18,  53, 192, 380,  13,  11,  31,  33,  35,\n",
      "         209,   0,   3,   0,  26,  52, 539, 337,   0,  18,   2,  24, 522, 192,\n",
      "          86,  12,  13,   0,  17, 111,   0, 496,  53,   0,  53,   0,   2, 114,\n",
      "           7,   0,  25,  31,   0,  75, 145,   3,  14,   2,   0,   0,   0, 838,\n",
      "           0,   3,   0,   0,  58,  98, 355,  67,   0,  63,   0,  38, 453,   0,\n",
      "           8, 730,  25,   2, 120,   5,   2,  24,  38, 161, 134, 132,  89,   8,\n",
      "           2,   0,   9,  87, 136,   0,   0,   0,   0, 264, 226,   2, 219,   0,\n",
      "          17, 422, 196,   2, 282,  24, 184, 102,   0, 349,   0,  18,  12, 196,\n",
      "          54,  44,   3,   0,  31,   7,  30, 239,  24,   0,   9, 256,  73,  53,\n",
      "           0,  11,  24, 105, 235,  42,  20,   4, 205, 630,   2,   0, 196, 828,\n",
      "         311,   0,   3, 156, 183, 115,  37,   0,   0,   6, 234,   0,  12,  53,\n",
      "         101,  26,  74, 132,   9,   0,  17,   0,   3,   0, 178, 462, 464,  18,\n",
      "          38,  87,  26,   4, 205,   6,  86,   0,  66,   0,   3, 363,  13,  58,\n",
      "           0, 561, 308,   7,   0,  75, 338,   2, 153, 166, 127,   3, 127, 252,\n",
      "          12, 150,   0, 192, 509,  11,  31,   6,   0],\n",
      "        [  9, 253,  30,   0, 437,   5,   0,   0, 425, 713,   3, 211, 248, 802,\n",
      "           7,  48,   0,  34, 395,  48, 366,   9, 345,   6,   2, 451, 662,   0,\n",
      "           3, 195,   0,   0,   0,   2, 461,   7,  42, 148,   0,  29, 126,  10,\n",
      "          67,  31,   8,   2, 250, 101,   0, 142, 110,  41, 274,  12,  42,  39,\n",
      "           6,  28,   0,  17, 585,   0,   7,  12,   0,  75, 842,  11,   0, 185,\n",
      "           9,  16,   0,   0,  94,   3,  48,   0,   0, 785, 281,   0,   4,   0,\n",
      "           6, 193,  60,  13, 811,   0, 203,   5, 167,   0,   2,   0,   5, 152,\n",
      "         290,  75, 130,  11, 117,   9,   0,   2,   0,   0,  15,   2,   0,   0,\n",
      "         483,   9, 253,  30,   0,   0,  17,   0,   4,   0,   0,   0,   2,   0,\n",
      "          56,   7,   0,  33,   0,   0, 503,   8,   0,   0,   2,   0,  67,   0,\n",
      "           0,   2,   0, 845,  76,   0,   2,   0,   0,   0,   2,   0,   0,   0,\n",
      "          17, 489,   2,   0, 176, 159,  73,  11, 185,   0,  88,  34, 100,   9,\n",
      "           0,   6,  73,  12, 493,  34,  86,   0,   4,   0, 124,  12,   3,  66,\n",
      "           0,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [235,  53,  20,   4, 205,   6, 278,  36, 766, 682,   0,  43,   0,   0,\n",
      "          18,  12,   7, 121,  47,  76,   4, 434,   0,  23,  37,   2,   0,   2,\n",
      "          24,   7,   0,  14,   0,   0,  14,   0,  14,  12,   0,   6,   0,   2,\n",
      "         114,   7,   4,   0,   5,   2,   0, 146,   0,   0, 821, 188,  74, 432,\n",
      "           6,   0,   0,   8,   2,   0,   5, 766, 682,   0,   9, 136,   2,   0,\n",
      "         114,  16,   0,  25,   4,   0,   0,   2,   0,   0, 189, 482, 222,  31,\n",
      "           6,   0,   3,   8, 789,   0,   0,   2, 785,   5,   2,   0,   0,   0,\n",
      "          31,   6,   0,  14,   4,   0,  61,   6,   0,  40,   0,  33,   2,  83,\n",
      "           0,   3,  31,   6,   0, 173,   0,  40,   0, 188, 143,  72,  97,   0,\n",
      "           0,  60,  76,   0,   0,   3,  40,   0,   0,   2,  60, 482, 760,   8,\n",
      "           2, 114, 495, 654,   0, 766, 682,   0,   0,   0,   0,  55,  20,   0,\n",
      "           0,  19,   0,   8,   2,  24,   7,   0,  25,  31,   5,   2,  90,   0,\n",
      "           0,   0, 102, 233,  21,   0,  17, 251,  53, 478,   6,  41,  11, 117,\n",
      "          12,  80,  26,  10,   0, 242,   6,  12,  10,   9, 300, 509,   3,  52,\n",
      "         539, 812,   5, 766, 682,   0,   0,  18,   2,  24, 522,   7, 148, 434,\n",
      "           6,   0,  99,   0,  76,   4,   0, 222,   3, 188, 265,   0,  97,   8,\n",
      "           2, 612, 766, 682,   0,   7,   4, 480,   5,  52, 787,  44,   0,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [  9,   0,  11,  23,   4, 286,   5, 137,   0,   3,  21, 343,   6, 163,\n",
      "          52,   0,  36,  12,   9, 195,  10,  65,   2,   0,   0, 192,  26,  12,\n",
      "           0,  10, 159,  26,  74,  35,   9, 538,   6,   0,  17,   0, 353,  41,\n",
      "         147,   7,  30,   0,  10, 334,  41,   4,   0,   5, 403,   0,   0,   0,\n",
      "           0,   0,   0,   0,  35,   0, 341,  25,   4,   0, 762,  10, 104,  53,\n",
      "         186,   6,   0,   8,  15,   2, 362,   5,   2, 212,   5,   2, 189,   0,\n",
      "           2,  61,  31,  10,   9,   0,  99,   0,   0,  37,  16,   2, 403, 271,\n",
      "           8,  69,   4, 140,   0,   0,   0,  46,   5,   2,   0,   8,   4, 517,\n",
      "           0,  11, 271,   0, 791,  13,   4, 162,   0,   5,   0,   3,  43, 592,\n",
      "           0,  60,  76,  10,  56,   7,  30, 271,  15,   4, 566, 429,   0,   3,\n",
      "           0,   3,  30, 271, 125,   4, 260,   0,   4,  54,   0,   0,   0,   0,\n",
      "           0,   0,   2, 288,   5,  11,   0,  17, 488,  39,  74,   0,   8,  60,\n",
      "           0,   2,   0,   0,   7,   0,   0,  56,   7, 197,   6,   0, 630,  45,\n",
      "         298,  75,   4,  44, 213,   0, 712,  46,   0,  50,   2,   0, 133, 759,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1]]). y is tensor([2, 2, 2, 2])\n",
      "tensor([1, 1, 1, 1])\n",
      "x is tensor([[ 11,  83, 164,   5,   2,   0,   0,  39,  47,   0,   3, 114,   0,  76,\n",
      "           0,  18, 308,   5,   2,   0,   5,   0,   0,   2,   0,   0,  26, 198,\n",
      "           6,   0,  62,  73,   2,   0,  75,   2,   0,  77,   2,   0,  57, 299,\n",
      "           6,   0,  34,  72,   5,   2,   0, 139,   0, 456,  55,   0, 360,   4,\n",
      "           0, 474,   6,   0,   4, 719, 315,   5,   0,   6,   0,  32,   0,  10,\n",
      "           0,   0,  87,   0,  72,   0,  51,  53,   0,   0,  17,   0,  52,   0,\n",
      "           8,   2,  83,   0,   2, 123,   8,   0,   0,  15,   2, 255,   0,   0,\n",
      "          25,   0, 701, 159, 169,   0,   2,   0, 123,  55,   0, 455,   0,   0,\n",
      "           0,   0,  55,  51,   0,   2,   0,   0,  12,  16,  34,   0,  18,  14,\n",
      "           2,  79,   0,   3,   2, 137, 169,   0,   9,  16,   0,  47,   3,  47,\n",
      "          71,  11,   0, 381,   0,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [  0, 743,   9, 418,  11,  33,   4,   0, 312,   3, 108,  20, 541,  13,\n",
      "          97,   2, 114,   7,   0,   0,   3,   2, 219,  24, 439,  41,  30, 271,\n",
      "           5,   4, 190,   0,  45,  29,  26,  99,   0,   5,   0,  50,   0, 124,\n",
      "           0,  29,  84, 242, 242,  41,   2,  24,   7,  30,   0,   6, 119,   0,\n",
      "         116,  17,   0, 588, 678,  72, 292,   0,   0,   0,   0,   2, 282,  96,\n",
      "           9,  26, 102,   0,   0,   0, 405,   0,  10,   0,   0,   7, 208,   0,\n",
      "         151,   2,  31,  10,   0,   2, 117, 116,  17, 251,   0,  10,   9,   0,\n",
      "         537,  81,   0,   5,  48, 139, 130,  11,   0,  43,   4,   0,  10,  38,\n",
      "         170,   6,   0,  12,   4,   0, 105,   9,  16,   4, 437,   5,   2,   0,\n",
      "          69,  16, 208, 143, 389,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [  2,   0, 576,   9,   0,   2,   0,   5,   0,   0,   0,   0,  58,   0,\n",
      "           0,   6,   4,   0,   0,  24,  36,   4,   0, 552, 437,   0,   0,  32,\n",
      "           0, 142, 284,   4,   0,   0,   0,   0,   0,  13,   2, 248,   6,   0,\n",
      "          17,   0,   4, 563,   0,  15,   0,   0,   0,   7,   0,   8,   0,   3,\n",
      "           0,  43,   0, 387,   5,   0,   0,   0,   0,   2,   0,   0,   0, 366,\n",
      "           0,  48,   0, 617,  23,   0,   3,   0,   0, 332, 487,   0,   0,  45,\n",
      "         298, 329,  13,   4,  24,  10, 186,   6,  28, 106,   5, 548, 443,  21,\n",
      "           4,   0,   0, 134,  67,   0,  18,  45, 298, 329,  13,   4,   0,   0,\n",
      "           0,   0, 453,  29, 159, 134,   4,   0, 347,   0,  17,   0,   0, 157,\n",
      "           0,   0,   3,  11,   7, 552, 149,   0, 631,   0,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [ 48,   0,   3,   9,   0, 211,   6, 124,  11,  23, 127,   4,   0,   5,\n",
      "           0, 275,   0,   3, 209,  62,   0,  49, 152,   0, 264,   2,  79, 170,\n",
      "         131,  12,  16,   8,   0, 228, 234,   0,   2, 123,   8,  69,  31,   5,\n",
      "           2, 138,   8,   0,   0,  25,   0,   0,  16, 654,   0,   3,   0,   2,\n",
      "         249,   5,   0, 141,   0,   0,   0,   0,   0,   2,   0,   5,   0,  16,\n",
      "           4, 358,   0,   3,   0, 167,   6,  73,  71,   2,   0,   5,   0,   0,\n",
      "           0,   2,   0,   5,   4,   0,   0,   0,  71,   4,   0,   0,   2, 609,\n",
      "          16, 448, 460,   3,   0,   8,   0,   2,   0,   0,   2,   0,   0,   0,\n",
      "           5,   0,   0, 123,   0,   0,   2,   0,   3,   0, 427,   5,   2,   0,\n",
      "           0,   0,  15,   2, 540, 687, 464,   6,   0,   4,   0,   3, 217,   0,\n",
      "           0,   2, 727,  69,   9,   0,   0,  13,  29, 629,   0,  48,   0,   3,\n",
      "           0,   5,   0, 158,   0,   0,   0,   2,   0, 666,  13,  43, 648,   0,\n",
      "           0,  10,  39, 642,  94, 473,   0,  75,   0, 724,  31,   6,   0,   0]]). y is tensor([1, 2, 1, 1])\n",
      "tensor([0, 1, 0, 0])\n",
      "x is tensor([[ 32, 101, 659,  ...,   0, 169,   0],\n",
      "        [  9, 217,  11,  ...,   1,   1,   1],\n",
      "        [  0,   9,  41,  ...,   1,   1,   1],\n",
      "        [ 15,  67, 287,  ...,   1,   1,   1]]). y is tensor([2, 1, 2, 2])\n",
      "tensor([1, 0, 1, 1])\n",
      "x is tensor([[  2,  61, 166,  ...,   1,   1,   1],\n",
      "        [  2,   0,   6,  ...,   3, 586,   0],\n",
      "        [ 11,  23,   7,  ...,   1,   1,   1],\n",
      "        [ 48, 177,   3,  ...,   1,   1,   1]]). y is tensor([2, 1, 1, 1])\n",
      "tensor([1, 0, 0, 0])\n",
      "x is tensor([[158,   5,   2,   0,   0,  16, 145,   8,   4, 244, 771,   5,   0, 213,\n",
      "         133,  10,  57,   0,  21, 683,   3,   0, 226,  10,   0,   5,  23, 276,\n",
      "           2,   0,  18,   0,  90,   5,   2,   0,  10,  16, 112,   0,   0,  11,\n",
      "          31,  16, 208,   4,   0,   0,   4,   0,   5,   0,   0, 723,   6,   0,\n",
      "         747, 158, 376,  15,   4,   0,   5, 592,   0,   0,   0,   3,   0,   0,\n",
      "          33,   2,   0,   0,   0, 119, 680, 175,   5,   0,   0,  27,   0,   6,\n",
      "           2,   0,  18,  30,   0,   0,   0, 570,  71,   4, 158,   5,   0,  14,\n",
      "          38, 218,   6,   0, 747, 158,  55,  38,   0,   0,   5,   0,   0, 723,\n",
      "           6,   0,   2,   0,   0,   0,  11, 158,   5,   0,   3,   0,  17,   0,\n",
      "         539, 213,  23,  10,   7,  82, 154,   8,   2,   0,   0,  45,  29, 124,\n",
      "           2,   0, 602,  89, 581,   0,  35,   5,   2,   0, 507,   3,   0,   0,\n",
      "           0,  13,   2,   0,   0,   4,   0,  23,  10,  16,   0,  25,  30, 510,\n",
      "           0,   0,  13, 213, 733,   0,  17,   0,   0,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1],\n",
      "        [462,  11,  24,  16,  25, 204, 845,  89,   2,  83,  31,   3,   2,  83,\n",
      "          31,   0,   2, 582, 107,  68,  16, 386,   0,   2,  96,   7,  54,   0,\n",
      "           3,  12,  92,  53,   0,   2,  83,   0, 160, 130,  11,   9,   0, 473,\n",
      "           0,   2,  83,  31,   7,   4,   0,   0,   6, 414,   9,  59, 278,   0,\n",
      "          12,  21,   0,  87, 670, 119, 457,  21,   0,  88,  20,   2, 282,  24,\n",
      "         184, 102, 154,  18,  88,  21,   2,   0,   9, 820, 130,  12, 275, 158,\n",
      "           3, 619,   0,  34,   9, 418,  12,   0,   3, 161, 619,   0,   0,   2,\n",
      "         114,  16,   0,   9, 454,   9,  66,   2, 228,  10,   4, 339,   0,   0,\n",
      "          27, 808,  21,   4,   0,   6,  66,   2, 225,   5,   2, 640, 582,   3,\n",
      "           0,  12, 174,  34,  38,  64, 237,  12,   6, 402, 290,   3, 295,   0,\n",
      "          18,  38, 106,  12, 125,  29, 478, 176,   6,   0,  38, 106,   0,   5,\n",
      "         100, 517,  32,  53,  57,  42,   0,  45,  29,   0,  94,  11,  16,   4,\n",
      "           0, 142,   5,   0, 614,   3,  20,   4,  44,   0, 142,   0,   2, 114,\n",
      "         101,  26, 106,  12,  45,   2,  96,  16,  44,   3,  45,  38, 208, 106,\n",
      "          29, 242,   0,   6,  73,   0, 321,   2,   0,  15, 147,   0,  48,   0,\n",
      "         101,  26, 345,   8,  10, 709,   3,   0,  46,   5,   4,   0,   9, 178,\n",
      "          12,   4,   0],\n",
      "        [ 11,   7,   4,  98, 498, 212,   4,   0,   0, 267,  15,  78,   0,   0,\n",
      "          69,   0, 787,  63,   0,   0,  64,  86,   6,   4, 140, 127,   4,   0,\n",
      "          14,   0, 680,   0,   0,   7, 385, 373,  15,  10,  90, 679,   0,   5,\n",
      "         498, 207,   0,   0,   0,   0,  15,   4,   0,   0,  46,   5,  19,   0,\n",
      "           7,   4,   0,   0,   3,   0,   0,  15,  19,   0, 355,   0, 110,   0,\n",
      "           0,   6,   2,   0, 209,  12, 186,   6,  28,   0,  14,   4,   0, 185,\n",
      "           9, 136,  12,  84, 163,  30, 254, 785,   5, 567,   0,  21,   2,   0,\n",
      "          10,   4, 205,   5, 122,  26,  74,  56, 432,  12,   3, 170,   2,   0,\n",
      "          91,   4,  98, 311, 563,  15,   4,   0, 573,   5,   0,  11,  53, 101,\n",
      "          28,   4,   0,   0,   0,   6,   2, 732,   0,   0,  41,   2,   0,   8,\n",
      "           2,   0,   0,  43,   4,   0,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1],\n",
      "        [103, 125,  86,   9,   0,  48,   0,   9, 345,   6,  11,  24,   0,  15,\n",
      "           4, 162, 353,  20, 639,  47,  76,   2, 207,  10,  57,   8, 274,   3,\n",
      "          10,  12,  16, 412,   6,  28,   4, 213,   0,  17,   0,   9,   0,  46,\n",
      "           0,   2,  83,   0,   0,  63,   4, 544,   0,   9,  49, 106, 131,  46,\n",
      "         338,  11, 117,   2, 114,  16,   0,   3,  34,  16,   2,   0,   2, 310,\n",
      "          57, 632,   6,   2, 198,  10, 122,   8,   2, 254,  57,   0,   0,  17,\n",
      "         111, 175, 258,  26,  74,  47,   0,   0,  65,  52,   5,   2, 266, 404,\n",
      "          41,  38, 159,  26,  74, 106,  72,   0,  38,   0,  21,  13,  67, 358,\n",
      "           0,  54, 544,   0,  17,   0,   8,  35,  11,  24,  16,   4,   0, 480,\n",
      "           5,  95,   3,   0,   0,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1]]). y is tensor([1, 2, 1, 2])\n",
      "tensor([0, 1, 0, 1])\n",
      "x is tensor([[ 13,  48,   0,  ...,   1,   1,   1],\n",
      "        [  9,  87, 136,  ...,   1,   1,   1],\n",
      "        [  2, 403, 375,  ...,   1,   1,   1],\n",
      "        [112,   4, 241,  ..., 530,  15, 477]]). y is tensor([1, 1, 2, 1])\n",
      "tensor([0, 0, 1, 0])\n",
      "x is tensor([[  9, 342,   0,  ...,   1,   1,   1],\n",
      "        [234,  41,   0,  ...,   1,   1,   1],\n",
      "        [  9,  26,   6,  ...,   1,   1,   1],\n",
      "        [ 14,   4, 772,  ..., 442,   0,   0]]). y is tensor([1, 2, 1, 1])\n",
      "tensor([0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for b, batch in enumerate(train_iter):\n",
    "    x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "    print('x is {}. y is {}'. format(x, y)) \n",
    "    print(y.data.sub_(1))      # lable 값 조정: y값에서 () 값을 뺌, sub 대신 add를 하면 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  9, 342,   0,  ...,   1,   1,   1],\n",
       "        [234,  41,   0,  ...,   1,   1,   1],\n",
       "        [  9,  26,   6,  ...,   1,   1,   1],\n",
       "        [ 14,   4, 772,  ..., 442,   0,   0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " x            # 현재 x 입력값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 427])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " x.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-5. Reviewing RNN Model\n",
    "\n",
    "* Embedding, RNN, Cost Function 동작 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2   # 분류되어야 할 결과 수 (긍정 or 부정)\n",
    "embed_dim= 128  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n",
    "hidden_size = 64  # RNN의 은닉층 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([847, 128])\n",
      "Parameter containing:\n",
      "tensor([[ 1.8423,  0.5189, -1.7119,  ..., -0.3012, -1.6497,  0.0287],\n",
      "        [ 0.3865,  0.9697,  0.1901,  ...,  0.3332, -1.0130,  0.6990],\n",
      "        [ 0.1949,  1.5408, -0.6603,  ..., -0.0864, -1.0778,  1.3174],\n",
      "        ...,\n",
      "        [-0.2979, -0.1140,  0.0030,  ..., -0.9771,  0.5036, -0.1678],\n",
      "        [ 0.5493,  0.7338, -0.2272,  ...,  1.6034,  0.0698,  0.4403],\n",
      "        [-0.0185, -0.1254, -1.0460,  ..., -1.0135, -2.4048,  1.2622]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# num_embedding는 trainset 단어 전체 갯수인 n_vocab로 지정\n",
    "Emb_Test=nn.Embedding(num_embeddings=n_vocab,   embedding_dim=embed_dim)\n",
    "\n",
    "# 가중치 확인\n",
    "print(Emb_Test.weight.shape)   # (단어갯수, embedding dim)\n",
    "print(Emb_Test.weight)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 427, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emb_Test(x).shape           # 임베딩 결과 차원 확인 : (batch 크기 x 문장 단어 크기 x embedding 크기)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN input Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000001CE2FB5B748>\n",
      "torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "rnn_test = nn.RNN(embed_dim, hidden_size, batch_first=True)      # GRU도 RNN과 동일\n",
    "\n",
    "# model에 구성된 파라미터 가중치 확인 : 추가 분석은 아래 참고\n",
    "print(rnn_test.parameters())\n",
    "print(next(rnn_test.parameters()).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 427, 64])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, hidden = rnn_test (Emb_Test(x))      # Tuple 형태의 결과를 분리\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8309, -0.5901,  0.8754,  ...,  0.0713, -0.9377, -0.1697],\n",
       "         [ 0.5309, -0.2628,  0.6628,  ...,  0.3199, -0.9549, -0.6521],\n",
       "         [-0.8563, -0.3423,  0.0986,  ..., -0.7848, -0.4433, -0.8328],\n",
       "         ...,\n",
       "         [-0.6477, -0.5515, -0.8874,  ..., -0.4215,  0.6261, -0.9349],\n",
       "         [-0.6477, -0.5515, -0.8874,  ..., -0.4215,  0.6261, -0.9349],\n",
       "         [-0.6477, -0.5515, -0.8874,  ..., -0.4215,  0.6261, -0.9349]],\n",
       "\n",
       "        [[-0.1611, -0.2976, -0.7103,  ...,  0.0046, -0.4604, -0.8281],\n",
       "         [-0.7050, -0.7122,  0.4462,  ..., -0.9862, -0.6149, -0.9758],\n",
       "         [-0.8419, -0.6081, -0.2817,  ..., -0.8651, -0.8230, -0.9076],\n",
       "         ...,\n",
       "         [-0.6477, -0.5515, -0.8874,  ..., -0.4215,  0.6261, -0.9349],\n",
       "         [-0.6477, -0.5515, -0.8874,  ..., -0.4215,  0.6261, -0.9349],\n",
       "         [-0.6477, -0.5515, -0.8874,  ..., -0.4215,  0.6261, -0.9349]],\n",
       "\n",
       "        [[-0.8309, -0.5901,  0.8754,  ...,  0.0713, -0.9377, -0.1697],\n",
       "         [-0.7831, -0.4973, -0.5416,  ...,  0.4515, -0.1665, -0.3808],\n",
       "         [-0.0997,  0.3240, -0.0656,  ...,  0.5262, -0.2649,  0.1318],\n",
       "         ...,\n",
       "         [-0.6477, -0.5515, -0.8874,  ..., -0.4215,  0.6261, -0.9349],\n",
       "         [-0.6477, -0.5515, -0.8874,  ..., -0.4215,  0.6261, -0.9349],\n",
       "         [-0.6477, -0.5515, -0.8874,  ..., -0.4215,  0.6261, -0.9349]],\n",
       "\n",
       "        [[ 0.5368,  0.4014, -0.6293,  ...,  0.3098, -0.1272,  0.3158],\n",
       "         [ 0.2691,  0.8694,  0.8378,  ...,  0.5023,  0.7967, -0.9611],\n",
       "         [-0.4895, -0.4414, -0.9530,  ...,  0.7161, -0.2483, -0.1657],\n",
       "         ...,\n",
       "         [-0.0918, -0.1227,  0.5026,  ...,  0.5447,  0.3870,  0.2745],\n",
       "         [-0.9442, -0.5933,  0.1864,  ..., -0.9186, -0.7073, -0.9289],\n",
       "         [-0.4579, -0.5915,  0.2738,  ..., -0.8681, -0.1956, -0.9544]]],\n",
       "       grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAACTCAYAAADWZv1dAAAa20lEQVR4Ae1dTa9e1XX20EOGHlqCn8BvKNNOShulA6bM/A/cKZ24ihRoq0pImaAOUoVEapUQQlsiOSWG2FjCRAhB0lbuDKlqa2Qbn+q56IHllX3ed3+dd6/9nmdLN/ucvdfHs5619l73Et/3Xlo0xIAYEANiQAxMxMClibAKqhgQA2JADIiBRY1LRSAGxIAYEANTMaDGNVW6BFYMiAExIAbUuFQDYkAMiAExMBUDalxTpUtgxYAYEANiQI1LNSAGxIAYEANTMaDGNVW6BFYMiAExIAbUuFQDYkAMiAExMBUDalxTpUtgxYAYEANiQI1LNSAGxIAYEANTMaDGNVW6BFYMiAExIAbUuE5cA0+ePFnufPy75Se/+GD54U/fy/768dvvL7fvfb48evzViRHLnRiYl4GHjx4vv7r9yfLmz29lnzWcyx+9dWu5+ZtPlgdfPuwSfA2OCBi24KIHoSdpXBEu65rC2SJpaFolDcvLQl/jPBnAJfnurY8vLk2f99T7P/zs18u//vre8r//9+V5EtIhKjStFHe5a9DvMVpwRMAAvnrh6MHnSRpXhMu6pXB6Jq30Jy1/wKCvcZ4MoGn5fOe8Q08jzUDpT1qeb+j3GC04ImAAL71w9ODzJI0rwmXdUjg9k+YPRs17j8TLRjwG8J+GauoBehppBmr49Dppy2Wr3mbpe5m3tHSpz5R82vLpV0/SuFIElK61UlPqLyXfigH6Kbulaz1wyEY8BkrrwMrHiyYGIstR7XOPSGp9Uy8CBmCJMtS4Cv6BRI+ksRBb5h44ZCMeA6qJ/jlp4ZS6PVDRVu0cAQOwRxlqXGpcUWpx9zhqL7VIF0q0JLZwSt0eMdFW7RwBA7BHGWpcalxRanH3OGovtUgXSrQktnBK3R4x0VbtHAEDsEcZalxqXFFqcfc4ai+1SBdKtCS2cErdHjHRVu0cAQOwRxlqXGpcUWpx9zhqL7VIF0q0JLZwSt0eMdFW7RwBA7BHGWpcalxRanH3OGovtUgXSrQktnBK3R4x0VbtHAEDsEcZalxqXFFqcfc4ai+1SBdKtCS2cErdHjHRVu0cAQOwRxlqXGpcUWpx9zhqL7VIF0q0JLZwSt0eMdFW7RwBA7BHGWpcalxRanH3OGovtUgXSrQk1n4aCXPR61NJWnBEwAA+euHoUSO7aVwthdMzafiUdx6Kmhn6GufJAD40t6YmoKeRZgCf8F7DKXV++f5v04YLV1twRMAAPnrhKKQuKX6SxhXhsm4pnJ5Jw58m4aGomT/46PNkIrU4PwP4pPeamvjnf/to/uA3igCfuI+zX/pZpfhGFxd1rz9rUoMjAgbUY28cPVJ9ksYV4bKuKZwtkoa/p3Xo73G99oM3k5cXmj+alv4eV4+yj2kDf54EzSv3vw7gJy00rf/+nwcxAwqO6sGDB8vVq1eX+/fvD0MaAQOCj4IjNxEnaVyzXNafffZZLm+byM1WPJuQIKNJBkbXZhLU5Is3btxYLl26tFy7dm1YJBEwIPgoOHITcZLGdQhMlMs6Ao7ZiudQXrXXj4EItdkvmhiWwOmVK1cuGtfly5eH/NQVAQOyEQVHSWUMb1xRLuvROGYsnpJCk2w9A6Nrsx55XM179+4t169fv2hcmG/evHlysBEwIOgoOEoSMLRxRbmsI+CYsXhKCk2ydQxEqM065HNo4T8Vjh4RMICDKDhy8jE0a1Eu6yg4ZiuenAKTTBsDkWqzLZKY2hEu6wgYkJ0oOHIqZWjjIsAohEXAEQED86I5DgOqi21yEYHXCBjAbhQcOZlW4zIsRUhcBAyGEj0GYUB1sU0iIvAaAQPYjYIjJ9NqXIalCImLgMFQoscgDKgutklEBF4jYAC7UXDkZFqNy7AUIXERMBhK9BiEAdXFNomIwGsEDGA3Co6cTKtxGZYiJC4CBkOJHoMwoLrYJhEReI2AAexGwZGTaTUuw1KExEXAYCjRYxAGVBfbJCICrxEwgN0oOHIyrcZlWIqQuAgYDCV6DMKA6mKbRETgNQIGsBsFR06m1bgMSxESFwGDoUSPQRhQXWyTiAi8RsAAdqPgyMm0GpdhKULiImAwlOgxCAOqi20SEYHXCBjAbhQcOZlW4zIsRUhcBAyGEj0GYUB1sU0iIvAaAQPYjYIjJ9NqXIalCImLgMFQoscgDKgutklEBF4jYAC7UXDkZFqNy7AUIXERMBhK9BiEAdXFNomIwGsEDGA3Co6cTKtxGZYiJC4CBkOJHoMwoLrYJhEReI2AAexGwZGTaTUuw1KExEXAYCjRYxAGVBfbJCICrxEwgN0oOHIyrcZlWIqQuAgYDCV6DMKA6mKbRETgNQIGsBsFR06m1bgMSxESFwGDoUSPQRhQXWyTiAi8RsAAdqPgyMm0GpdhKULi3nnnHYNIj2LgawYi1OY55iICrxEwILdRcOTU2eaN66snj5f3/uvvl7+9++fL927/cfHX39z97vLuf76+wE7taMUA3OeEo5ZH6W3DQGt99qjNbSIbazUCr60YdPeka2jzxoWm9eqdF4sblm1y0P+X//i7dAQZqz0wAM+54Mig7KQiONxv//v3l9c+/LOqOnn1wxeXt37/veXhVw+qcI/236M+W2uzirjgShF47YFBd88fFtrmjav2Jy3buPCMS6129MJwLjhqedxKD03r1Tt/UtW0WCevfviny89+/1dVEEf771WfLWekirjgShF47YVBd8/TxbZ54+LF0mN+Gnr+Ww/f1ka+56clrY0ez09bn/et9ictzyGaV80Y7d/H0fJeE/+56rTw6HVrOfJ2Wt9nx1GL3+upcVX8/26exNz31qL1+rl+o8v5uFrea2Jt8ed1Z/Rfg3kGHZ+blvfaeFt8pnRnx1GL3+upcalx+Zo4+XvqgNau1YCv9ZXSm9F/DeYZdFL5qV2rjbfW35re7Dhq8Xu9s2xcd+/eXR49evRNrGtFULv+jeEjD1FwHIE5fLs2Dym9nGDu3bu3PHjw7T/kSNmpXZvBfw7Gc5CpzWFKL5ePKGc+Co5c3krlzrJxvfTSS8tzzz23vP766xcNLFWILWu5JEfBkYt3lFxLLrxuTgzXr19frly5sty4ceOigXkbLe8z+M/BeA4yLXn0url8RDnzUXDk8lYqN1Xjwi/IlX49//zzTf9azRcw3ksxQH4LHKXJjiqf4rh2rTQ3V69e7Vofo/1HzfEIXLU1lNIrzetWZz4KjhH5tD6nalwW+KFnfLfxzDPPLPjO+osvvuh6MaGoc0cUHLl4R8mlLoratZwYUBeXL19erl27tty/f79rfczgPwfjOcjU1lBKL5ePKGc+Co5c3krlzrJx4WOT0LA4UoXYska7x+YoOI7hHL3fkguvmxPLzZs3LxoWZb2NlnfaPDSP9n8I2zntteTR6+byEuXMR8GRy1up3Fk2Lk+CL8LWd28/973Vr9fP9RtdzsfV8l4Ta4s/rzuj/xrMM+j43LS818bb4jOlOzuOWvxeT41L/xze18TJ31MHtHatBnytr5TejP5rMM+gk8pP7VptvLX+1vRmx1GL3+tt3rjwAaBrSShZ/+u73/HYs997YQDec8CRTdyJBPFZgyW1sCZb+8kZo/33qs+W2jxRqk/qJgKvvTDo7nm6dDZvXPhk99YP2f3+nRcvPoT1aej5bz0woHDOBUc+c6eRxAfktjaP1z78zvJPn/1lFeDR/nvUZ2ttVhEXXCkCrz0w6O75w0LbvHHhk7fxye74bnDtO+VD69DDh6DCTu1oxQB854Sjlset9PCp7viAXPzEtFYLf/GPf7S6Bz00rdpPhx/tP6c+D8Xfoza3yu1IuxF4bcWguyddQZs3rrTbb1fxCQb4XRr8s2QNMZBiYHSN7N1/KifnsDY6r+AwAoZIOHLranjjwqcX4Jfq8Ds1GmIgxcDoGtm7/1ROzmFtdF7BYQQMkXDk1tXQxoXvNvDRO2hc+IVQ/dSVm7b9yI2ukb37P9dKG51X8BoBQyQcJbU2tHHhw07xKQZoXJjxi5kaYsAyMLpG9u7f5uKcnkfnFVxGwBAJR0l9DW1cBIrGpSEGDjEwukb27v9QbmbeG51XcBcBQyQcOfUUomNESVwOYZIZw8DoGtm7/zFZ397r6LwiwggYIuHIyboaVw5LkhnOwOjDvXf/wwtgIwCj84qwImCIhCMn1WpcOSxJZjgDow/33v0PL4CNAIzOK8KKgCESjpxUq3HlsCSZ4QyMPtx79z+8ADYCMDqvCCsChkg4clKtxpXDkmSGMzD6cO/d//AC2AjA6LwirAgYIuHISbUaVw5LkhnOwOjDvXf/wwtgIwCj84qwImCIhCMn1WpcOSxJZjgDow/33v0PL4CNAIzOK8KKgCESjpxUq3HlsCSZ4QyMPtx79z+8ADYCMDqvCCsChkg4clKtxpXDkmSGMzD6cO/d//AC2AjA6LwirAgYIuHISbUaVw5LkhnOwOjDvXf/wwtgIwCj84qwImCIhCMn1WpcOSxJZjgDow/33v0PL4CNAIzOK8KKgCESjpxUq3HlsCSZ4QyMPtx79z+8ADYCMDqvCCsChkg4clKtxpXDkmSGMzD6cO/d//AC2AjA6LwirAgYIuHISbUaVw5LkhnOwOjDvXf/wwtgIwCj84qwImCIhCMn1WpcOSxJZjgDow/33v0PL4CNAIzOK8KKgCESjpxUq3HlsCSZ4QyMPtx79z+8ADYCMDqvCCsChkg4clKtxpXDkmSGMzD6cO/d//AC2AjA6LwirAgYIuHISfVJGtfDR4+XX93+ZHnz57eWH/70vayvH711a7n5m0+WB18+zIlDMpMzMLpG9u5/8vJZhf/kyZPlzse/W37yiw+y7h3eTz9++/3l9r3Pl0ePv1q1nbsRAQOwRsGRy9shuZM0LjQtFkTpDF2N82dgdI3s3f+5VhiaVumdY+Wh3zoiYEAMUXC08gn9kzSukp+0bNHgGboa58/A6BrZu/9zrbDSn7T8/QP91hEBA2KIgqOVT+ifpHH5Yih97xGobMRmoLQmvHxrdN5e6fvs/lvxR9UvzWNKvjW2lM3StVYM0C/1mZLvgaOHDTWuHizKRjMDqUNSstYKoMRXSnZ2/634o+qnclW61hpbqb+UfCsG6Kfslq71wNHDhhpXDxZlo5mB0gPk5VsBeHul77P7b8UfVb80jyn51thSNkvXWjFAv9RnSr4Hjh421Lh6sCgbzQykDknJWiuAEl8p2dn9t+KPqp/KVelaa2yl/lLyrRign7JbutYDRw8balw9WJSNZgZKD5CXbwXg7ZW+z+6/FX9U/dI8puRbY0vZLF1rxQD9Up8p+R44ethQ4+rBomw0M5A6JCVrrQBKfKVkZ/ffij+qfipXpWutsZX6S8m3YoB+ym7pWg8cPWyocfVgUTaaGSg9QF6+FYC3V/o+u/9W/FH1S/OYkm+NLWWzdK0VA/RLfabke+DoYUONqweLstHMQOqQlKy1AijxlZKd3X8r/qj6qVyVrrXGVuovJd+KAfopu6VrPXD0sKHG1YNF2WhmoPQAeflWAN5e6fvs/lvxR9UvzWNKvjW2lM3StVYM0C/1mZLvgaOHDTWuHizKRjMDqUNSstYKoMRXSnZ2/634o+qnclW61hpbqb+UfCsG6Kfslq71wNHDhhpXDxZlo5mB0gPk5VsBeHul77P7b8UfVb80jyn51thSNkvXWjFAv9RnSr4Hjh42TtK48EnvKRJy1qCrcf4MjK6Rvfs/1wrDp7zn3DNrMtBvHREwIIYoOFr5hP5JGhf+PMlaYRxb/+X7v+0Rp2wEZ2B0jezdf/DyqIaHP01y7I45tP/BR59X+6ZiBAzAEgUHeWmZT9K48De1cDGUfAI3vgNG09Lf42pJ7zy6o2tk7/7nqZQypPh7WvhzHqWfjI6fTtC0evw9rggYwFoUHGUZTEufpHGlXWtVDIgBMSAGxEA5A2pc5ZxJQwyIATEgBgYyoMY1kHy5FgNiQAyIgXIG1LjKOZOGGBADYkAMDGRAjWsg+XItBsSAGBAD5QyocZVzJg0xIAbEgBgYyIAa10Dy5VoMiAExIAbKGVDjKudMGmJADIgBMTCQATWugeTLtRgQA2JADJQzoMZVzpk0xIAYEANiYCADalwDyZdrMSAGxIAYKGdAjaucM2mIATEgBsTAQAbUuAaSL9diQAyIATFQzoAaVzln0hADYkAMiIGBDKhxDSRfrsWAGBADYqCcATWucs6kIQbEgBgQAwMZUOMaSL5ciwExIAbEQDkD2Y3r5ZdfXi5duvTNF119+umny7PPPsvX5Ox1X3jhhQs52MPA/rvvvpvUxeIbb7zxjV+PAXq0t2pAG5sxAO5tTvCMfGJgRu4OjVdeeeUpfdYSZtQW9o/ZgH2Lgf6xDn18acRnYHQt4S6xdcRn1GHOPRef4fNBmNW4cPDtZYAE84LJSejaBYbCwMD+scZl/Vv61xqXxWjlt3qe8YLERZHTFA5xBhtruVvLu7W3xltJ44KsxQC/bFYp+6hZ1p7FMvJ5rY5HYbIcngrD6Fo6lIO1e0611Kc6cA8h/7kjq3GhiO3FAOPRGxfx5RLRKpe6IFttnkK/lafRlw048jGgVlGzGKm8ePlT8HzMx6FL85juVvvILS7mU43RtXQoB2uNS7XUrzpwVnO/kc5qXDDIiwAwkWAmbC2hNhzopgDxu95UY7T60LX+7V6q2DwBkOGP/ZjtYcQ75LlvcVq9Q98NWH1rHzq0u4afsWDf2mEM1Le48Mx1axc58TZgn7bpy+aMfrhXOo++bIAXcSNXHDZexIcvDnDHd/IAefIJOdjDO2ucujaf9IcZ63YPdvEFG3bAj80j92ydQYcywElcHgt17Wzrwvpew0hd64fcYM9yRdkt59G1RJ5SMbJW7J7lh/tRa2mtNmw8fIYs4uA5QF5Yz6gr7NnBGrU1hzqydcX6hW3YswN7sI8vv2fl7PPTJ8vuuGebEAuQCXPiT71CF4D9sAGjaNYGiUzt20PPoDlDnoRTl/J8BwYeVuyRYOxjD/oYiIFy1LUzE8U1Lw9Mh/QhT9/EzDg8LshyWIzQ5x5tQA76tIV3ixVy1KHNkhl2YT81YJc5TuUfOhaLtYFYjulaecpitvF4+9hjTskRsREv40Fs3MPMdTyTT6zBJ/fgj/49N5BbG9CnTchYH3iHXbvv7UCf9UN9vh/C6PWggzWOQz4p02v2fFm7zA04ZE7sPp59rrmfW0uIey1e1gr8k9dZasnnGPwxBnJkZ+wjTp4T8kcZu8daxx6ekQMMzJDjAK/Mm133nK/xTzucv7XMlQ1mBETQ1jwDwL49LFYGz9C1BNl9Hzj2bFJSxWz9EQNtUhc+mQTsIYmHSPV+vN1j+p4j+LKcEBdxcrZykGGxYR97fLd7dh1ya7bp49Bs/Xs5H5Pfx7vnjTLEi/1U7VCudLaxghv77mvJ1wB9WT2vgz1wgmHrFnJrNQxZbyfFq8VKLJxTXDPP3rbFCD3sc/iYD/mkTq85FTNtp+LjHufWWvI80S5mm3OuW278vrfleaUNq+d1bJ5aainFHWuDOOxsfWHd8+prhrpWz+vY+C0ef77XbNMH5+zGBVJxGae+bAJp2M4WqF3n5X4MrCXE6uPZJxtrFk/Kt10jBtqFLgoGhPpYaRcz9yCHYRNli5F27RqKhvrAgmEx4d0fYuLCnscGDjCsjLcBHfBoD8OFkuOLa7mzx2n1fEx2j8+WN65hZizEbfdSz/BFTv0MGxywy2FzgjVfS77uoEvbtON1vE3KAR/zZLGCv5Rv6MGWHVxDPB5HKg9cO4QRMrTFGfg4iJ/vW87Em/IBTMjHodFaS54n68vnFXuWG7/vbW1VS6la8L5TvHINuJh3zBgeq+fV5gJxW33WjtexNoGPcpZD+MY69o+N7Ma1ZsgnLCUHMDY4PAMciToG1pNLWyQfsx2WDE8g5Kw/YqA+dBETfEI3d3g/3i5sepzWNjDBJwdj4ztxwQ9kOawcZVJ79A996weyli/q5s7wz3zYGT58TCmbwGP18AxdxpLCm7KztgZ9fHHYWH3t+gPPGKALPR4oq+d17B70WGvWL7HY2duxeaXcIRsprmEDeLxti5H46MPPh3x62db30bUEnnwt4h0cWM4Yp+XG73vOR9bSodpgLHa2WLHuzxDtIWbww2H1vI7dgzy4A0ewZcexeqTst165sjKvJZWJXVE7uMygc8GmjPkCgQwOAAdx23dbcMTAfRapTwre7QVIec4+McBg5fEOmbXBYuA+5IGdg7hsQTA2ylGGOt4G3vFlB+LyxWP3W559TCW2GAviPcQbbcIXcpn6snmAHGLGwAw/HODR8mNzSjyQhQ3qeZ01mxYD/dnZ60HeYsE7/K4NYCUmyOCd+ocwWjnqsZ7wThtrfk+1jtiBtWYwd+Cw1obPD3DMUkuIea02UnxC3tYaeMMXB3PhOYEP6nmdlE3Ulq012M+tt6LGlWuUAR6b2TQQrA/gmC73/aHEOkiz9kCavdCoi5kYuMYix7vVs4mnrJ/pAwnFgA7XbOK9Ht5ZDNzzSbW4aBMyVs7KwI7dw7svHq5hfYvhYyrxwVjAWw6+XF+wRXv+4PlaghzsYuCZvAMT8GF4HW8TMozlQuHA/yBf8EF88E2f2Ds2gIvyxAedYxitH8ZLvWN1ewxTr33gIi+lNsl/bi2l7KfyCjzE5Pc955Ajt3hmnk5VS2u1kYrVYsU+dPHFYXOBZxsLY/Q63qbni7Zz6hyyRY2LAFMzgJQO2MFAsEh0zfAFAhvAkktAjc9ZdVI82wuud1zwh4KtGaWXDXyl6hJrvha2jNnHOnMtgreac+056PF+ylpK4QUPqbpJraX0e6zNXEs+ftwLthliH++590V24/KOo7+DBBS7xtcMpA4eLqbcQjknHhGzb2Zbxddy4W6FKccucPuLJUdvbzKqpbqM8xtTapfyeLaNi4Ro/vonWvzkUftTrTgsZwDfKKR+2iu3JI29M3BOtYQGhXPR+k2RGtfeT4XiFwNiQAxMxoAa12QJE1wxIAbEwN4ZUOPaewUofjEgBsTAZAyocU2WMMEVA2JADOydATWuvVeA4hcDYkAMTMaAGtdkCRNcMSAGxMDeGVDj2nsFKH4xIAbEwGQMqHFNljDBFQNiQAzsnQE1rr1XgOIXA2JADEzGgBrXZAkTXDEgBsTA3hlQ49p7BSh+MSAGxMBkDPw/m5j/2gZXTT0AAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return Analysis\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "* 리뷰 감성 분류는 긍정/부정 하나의 분류이므로, RNN 다대일 구조이며, 이 경우 RNN 연산 결과는 n개 은닉 상태 중에서 마지막 번째만 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6477, -0.5515, -0.8874,  0.9360,  0.3518, -0.3544,  0.1969,  0.7040,\n",
      "          0.7838,  0.9015,  0.3021,  0.2159, -0.6491,  0.3018, -0.5585,  0.5743,\n",
      "         -0.0343, -0.4258,  0.8133, -0.2039,  0.9046, -0.9173, -0.7659,  0.8209,\n",
      "          0.6063, -0.7208, -0.0690, -0.7470,  0.2297,  0.9158,  0.4551, -0.8675,\n",
      "         -0.7099,  0.5056, -0.8337,  0.5576,  0.6347, -0.0290,  0.3234,  0.8656,\n",
      "         -0.1665, -0.6618, -0.0930, -0.2165, -0.4128,  0.3312, -0.8838, -0.5811,\n",
      "          0.8708,  0.8002,  0.0334, -0.6721,  0.1329,  0.9255, -0.5492, -0.8813,\n",
      "         -0.1483, -0.3118, -0.8593, -0.6132, -0.9289, -0.4215,  0.6261, -0.9349],\n",
      "        [-0.6477, -0.5515, -0.8874,  0.9360,  0.3518, -0.3544,  0.1969,  0.7040,\n",
      "          0.7838,  0.9015,  0.3021,  0.2159, -0.6491,  0.3018, -0.5585,  0.5743,\n",
      "         -0.0343, -0.4258,  0.8133, -0.2039,  0.9046, -0.9173, -0.7659,  0.8209,\n",
      "          0.6063, -0.7208, -0.0690, -0.7470,  0.2297,  0.9158,  0.4551, -0.8675,\n",
      "         -0.7099,  0.5056, -0.8337,  0.5576,  0.6347, -0.0290,  0.3234,  0.8656,\n",
      "         -0.1665, -0.6618, -0.0930, -0.2165, -0.4128,  0.3312, -0.8838, -0.5811,\n",
      "          0.8708,  0.8002,  0.0334, -0.6721,  0.1329,  0.9255, -0.5492, -0.8813,\n",
      "         -0.1483, -0.3118, -0.8593, -0.6132, -0.9289, -0.4215,  0.6261, -0.9349],\n",
      "        [-0.6477, -0.5515, -0.8874,  0.9360,  0.3518, -0.3544,  0.1969,  0.7040,\n",
      "          0.7838,  0.9015,  0.3021,  0.2159, -0.6491,  0.3018, -0.5585,  0.5743,\n",
      "         -0.0343, -0.4258,  0.8133, -0.2039,  0.9046, -0.9173, -0.7659,  0.8209,\n",
      "          0.6063, -0.7208, -0.0690, -0.7470,  0.2297,  0.9158,  0.4551, -0.8675,\n",
      "         -0.7099,  0.5056, -0.8337,  0.5576,  0.6347, -0.0290,  0.3234,  0.8656,\n",
      "         -0.1665, -0.6618, -0.0930, -0.2165, -0.4128,  0.3312, -0.8838, -0.5811,\n",
      "          0.8708,  0.8002,  0.0334, -0.6721,  0.1329,  0.9255, -0.5492, -0.8813,\n",
      "         -0.1483, -0.3118, -0.8593, -0.6132, -0.9289, -0.4215,  0.6261, -0.9349],\n",
      "        [-0.4579, -0.5915,  0.2738,  0.5096,  0.3285,  0.8361, -0.0651,  0.9138,\n",
      "         -0.9158,  0.7690,  0.9270, -0.9829,  0.1577, -0.4004, -0.1772, -0.2277,\n",
      "          0.9021, -0.6047, -0.8536, -0.0071, -0.0077,  0.8052,  0.8391, -0.9434,\n",
      "          0.7268, -0.8836,  0.7484,  0.4093, -0.4668, -0.2740, -0.1836,  0.7792,\n",
      "         -0.6763,  0.7387,  0.7663,  0.8554, -0.3555, -0.7054,  0.5753, -0.5362,\n",
      "         -0.6339, -0.1594,  0.4839, -0.5469,  0.4680, -0.7207, -0.1299, -0.0191,\n",
      "         -0.9293,  0.7094, -0.2742,  0.7605,  0.9075,  0.9285,  0.7017,  0.4784,\n",
      "         -0.0548,  0.8917,  0.1549,  0.6724,  0.2505, -0.8681, -0.1956, -0.9544]],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "x_out = output[:, -1, :]\n",
    "print(x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2954, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.3938,  1.4080,\n",
      "          1.5675,  1.8031,  0.6043,  0.4318, -0.0000,  0.6037, -1.1171,  0.0000,\n",
      "         -0.0686, -0.8517,  0.0000, -0.0000,  1.8093, -0.0000, -0.0000,  1.6419,\n",
      "          1.2125, -0.0000, -0.1379, -0.0000,  0.4593,  1.8316,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000, -0.0000,  0.0000,  1.2694, -0.0580,  0.0000,  0.0000,\n",
      "         -0.0000, -1.3236, -0.0000, -0.0000, -0.8256,  0.6625, -0.0000, -1.1622,\n",
      "          0.0000,  0.0000,  0.0668, -1.3441,  0.0000,  1.8511, -1.0984, -1.7626,\n",
      "         -0.0000, -0.6235, -1.7187, -1.2264, -0.0000, -0.8431,  1.2522, -0.0000],\n",
      "        [-1.2954, -0.0000, -0.0000,  0.0000,  0.7036, -0.0000,  0.3938,  1.4080,\n",
      "          1.5675,  1.8031,  0.6043,  0.4318, -1.2982,  0.0000, -1.1171,  0.0000,\n",
      "         -0.0000, -0.8517,  1.6265, -0.0000,  1.8093, -1.8346, -0.0000,  1.6419,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  1.8316,  0.9102, -0.0000,\n",
      "         -0.0000,  1.0113, -1.6675,  0.0000,  0.0000, -0.0580,  0.6468,  0.0000,\n",
      "         -0.3330, -1.3236, -0.0000, -0.4330, -0.0000,  0.6625, -0.0000, -1.1622,\n",
      "          1.7415,  1.6004,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -1.7626,\n",
      "         -0.2965, -0.6235, -0.0000, -0.0000, -1.8578, -0.8431,  0.0000, -0.0000],\n",
      "        [-1.2954, -1.1030, -0.0000,  0.0000,  0.7036, -0.7089,  0.3938,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -1.1171,  1.1486,\n",
      "         -0.0000, -0.8517,  0.0000, -0.4078,  1.8093, -0.0000, -1.5318,  1.6419,\n",
      "          0.0000, -1.4417, -0.0000, -0.0000,  0.0000,  1.8316,  0.9102, -0.0000,\n",
      "         -0.0000,  0.0000, -1.6675,  0.0000,  1.2694, -0.0580,  0.0000,  1.7312,\n",
      "         -0.3330, -0.0000, -0.1860, -0.4330, -0.0000,  0.6625, -1.7675, -1.1622,\n",
      "          0.0000,  0.0000,  0.0000, -1.3441,  0.2658,  0.0000, -0.0000, -1.7626,\n",
      "         -0.0000, -0.6235, -0.0000, -1.2264, -1.8578, -0.8431,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  1.6722, -0.0000,  0.0000,\n",
      "         -1.8317,  1.5380,  0.0000, -1.9657,  0.0000, -0.8009, -0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -1.7073, -0.0143, -0.0153,  1.6104,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  1.4967,  0.0000, -0.0000, -0.5479, -0.0000,  0.0000,\n",
      "         -1.3525,  1.4775,  1.5326,  0.0000, -0.0000, -1.4109,  1.1505, -1.0724,\n",
      "         -0.0000, -0.3189,  0.0000, -1.0938,  0.0000, -1.4414, -0.2598, -0.0383,\n",
      "         -1.8586,  0.0000, -0.5484,  1.5211,  0.0000,  1.8569,  0.0000,  0.0000,\n",
      "         -0.1096,  1.7835,  0.0000,  1.3448,  0.5009, -0.0000, -0.3912, -1.9088]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dropout_p=0.5         # '0'은 dropout이 없고, '1'이면 모두 zero로 만듬\n",
    "dropout = nn.Dropout(dropout_p)\n",
    "x_out = dropout(x_out)\n",
    "print(x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression test for Binary Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2443, -0.5015],\n",
       "        [ 0.5286, -0.4763],\n",
       "        [ 1.1918, -0.4690],\n",
       "        [-0.1570, -0.3682]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_test = nn.Linear(hidden_size, n_classes)\n",
    "out=linear_test(x_out)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0351,  0.0067, -0.0173, -0.0635,  0.0814,  0.0863, -0.0431, -0.0899,\n",
       "         -0.0134,  0.0978, -0.0422, -0.0151,  0.1090, -0.0160, -0.0334, -0.0112,\n",
       "         -0.0052,  0.0647,  0.0691, -0.0737, -0.0336, -0.1198, -0.0648,  0.0670,\n",
       "         -0.0921, -0.0759, -0.0680,  0.1029, -0.0674,  0.1144,  0.0967,  0.1032,\n",
       "         -0.1186, -0.0465, -0.0963,  0.0907,  0.0992,  0.1186, -0.0615,  0.1202,\n",
       "         -0.0750,  0.1171, -0.0396,  0.0631,  0.0475, -0.0762, -0.0116, -0.1097,\n",
       "         -0.0971, -0.0257, -0.1155,  0.1131, -0.1057,  0.0859,  0.1166, -0.0639,\n",
       "         -0.0426, -0.1192,  0.0105, -0.0587,  0.0616, -0.0389, -0.0465,  0.0014],\n",
       "        [ 0.0083,  0.0878, -0.1011, -0.1065, -0.0709, -0.0362, -0.0658, -0.0083,\n",
       "         -0.0487, -0.0203,  0.1249,  0.0667,  0.0062, -0.0700,  0.0802, -0.0187,\n",
       "         -0.0455, -0.0304,  0.0104,  0.0442,  0.0819,  0.0046,  0.0354,  0.0593,\n",
       "          0.0963, -0.0602,  0.0751, -0.0318, -0.0894, -0.0595, -0.0233,  0.0371,\n",
       "          0.0962,  0.0197, -0.1101,  0.1150,  0.1210,  0.1130, -0.0607, -0.0552,\n",
       "         -0.0739,  0.0535, -0.0661, -0.1093, -0.0305,  0.0123,  0.0547,  0.0409,\n",
       "          0.0603, -0.1028, -0.0111,  0.0930,  0.1018,  0.0654,  0.0363,  0.0865,\n",
       "         -0.0029, -0.0299,  0.1118, -0.0335,  0.1186,  0.0705, -0.1182,  0.0778]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 확인\n",
    "print(linear_test.weight.shape)\n",
    "linear_test.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6180, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(out, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-6. Designing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Modeling\n",
    "\n",
    "* 연산 순서: Embedding -> GRU (or RNN) -> binary Classification\n",
    "* 긴 문장이되면 동일 하이파라미터 조건에서도 RNN보다 GRU가 우수하므로 아래는 GRU로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel(nn.Module):\n",
    "    def __init__(self, hidden_size, n_vocab, embed_dim, n_classes, dropout_p, batch_first=True):    \n",
    "        super(myModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=n_vocab, embedding_dim=embed_dim)\n",
    "        self.gru_layer = nn.GRU(embed_dim, hidden_size, batch_first=batch_first)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.linear = nn.Linear(hidden_size, n_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.embedding_layer(x)\n",
    "        output, hidden = self.gru_layer(output)\n",
    "        x_out = self.dropout(output[:, -1, :])\n",
    "        output = self.linear(x_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "simple_model = myModel(hidden_size, n_vocab, embed_dim, n_classes, dropout_p, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2748,  0.2197],\n",
       "        [ 0.1230,  0.0058],\n",
       "        [ 0.4120, -0.5067],\n",
       "        [-0.3325,  0.5508]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model(x)           # x 입력하여 Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=simple_model.parameters(), lr = 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/20] output is tensor([[ 0.3669, -0.2164],\n",
      "        [ 1.5354, -0.5557],\n",
      "        [-0.2669, -0.1901],\n",
      "        [-0.4342, -0.3179]]), y is tensor([0, 1, 0, 0]), and loss is 4.1362 \n",
      "[08/20] output is tensor([[-2.3515,  2.5728],\n",
      "        [-2.3579,  2.8697],\n",
      "        [-2.2347,  2.0447],\n",
      "        [-2.6190,  2.5609]]), y is tensor([1, 1, 1, 1]), and loss is 0.0320 \n",
      "[12/20] output is tensor([[-2.5561,  3.4146],\n",
      "        [-2.6607,  1.9399],\n",
      "        [ 3.2835, -2.9064],\n",
      "        [-4.5652,  4.3052]]), y is tensor([1, 1, 0, 1]), and loss is 0.0147 \n",
      "[16/20] output is tensor([[ 3.5604, -3.2366],\n",
      "        [ 2.6614, -3.4598],\n",
      "        [ 3.2810, -1.6573],\n",
      "        [-2.4014,  3.7117]]), y is tensor([0, 0, 0, 1]), and loss is 0.0127 \n",
      "[20/20] output is tensor([[ 2.8117, -3.5328],\n",
      "        [-4.3522,  4.1144],\n",
      "        [-2.5390,  2.1856],\n",
      "        [ 1.4785, -2.5598]]), y is tensor([0, 1, 1, 0]), and loss is 0.0283 \n"
     ]
    }
   ],
   "source": [
    "for step in range(1, 21):\n",
    "        for b, batch in enumerate(train_iter):\n",
    "            x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "            y.data.sub_(1)  # 레이블 값을 0과 1로 변환\n",
    "            optimizer.zero_grad()\n",
    "            output = simple_model(x)\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if step % 4 == 0:\n",
    "            print(\"[{:02d}/20] output is {}, y is {}, and loss is {:.4f} \".format(step, output.data, y, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss is 3.159780145475739, val accuracy is 55.26315689086914\n"
     ]
    }
   ],
   "source": [
    "corrects, total_loss = 0, 0\n",
    "for b, batch in enumerate(val_iter):\n",
    "    x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "    y.data.sub_(1)\n",
    "    output = simple_model(x)\n",
    "    loss = F.cross_entropy(output, y, reduction='sum')\n",
    "    total_loss += loss.item()\n",
    "    corrects += (output.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "size = len(val_iter.dataset)\n",
    "avg_loss = total_loss / size\n",
    "avg_accuracy = 100.0 * corrects / size\n",
    "print(\"val loss is {}, val accuracy is {}\".format (avg_loss, avg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고: Model의 가중치(Parameters) 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001CE2FB5BBC8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 가중치\n",
    "simple_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1545, -1.0168, -0.8931,  ..., -0.0169, -1.2699,  0.8142],\n",
       "         [-0.1780,  0.3700,  0.7450,  ..., -0.5840,  0.8755, -0.3447],\n",
       "         [-1.5603, -1.1133,  1.4238,  ...,  1.3853,  0.7623,  0.6323],\n",
       "         ...,\n",
       "         [ 0.1791, -1.3558,  0.7006,  ..., -2.8928,  0.4766,  0.2899],\n",
       "         [ 1.6682,  0.6897, -0.6469,  ..., -0.9862,  1.3039, -0.8010],\n",
       "         [-0.7036,  0.2249,  0.1881,  ...,  0.7784,  0.6586,  0.1922]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0077,  0.0372, -0.0958,  ..., -0.0263,  0.0365,  0.0744],\n",
       "         [-0.0189, -0.0125, -0.0102,  ...,  0.1086,  0.0727, -0.1115],\n",
       "         [ 0.2311, -0.0953, -0.1163,  ..., -0.2726,  0.1435,  0.0790],\n",
       "         ...,\n",
       "         [-0.0707,  0.0402, -0.1084,  ...,  0.1310, -0.0515,  0.0884],\n",
       "         [-0.0569,  0.1675,  0.1259,  ...,  0.0121,  0.0911, -0.1529],\n",
       "         [ 0.0524, -0.0799,  0.0901,  ...,  0.0204, -0.0948, -0.0949]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0022,  0.0024,  0.0410,  ..., -0.0845,  0.1769,  0.0106],\n",
       "         [ 0.0108,  0.0209,  0.1095,  ..., -0.0576,  0.0137,  0.0643],\n",
       "         [-0.0519, -0.1007, -0.0927,  ...,  0.0857,  0.1398, -0.0810],\n",
       "         ...,\n",
       "         [ 0.0661, -0.0334, -0.0176,  ..., -0.0208, -0.0104,  0.0167],\n",
       "         [ 0.0985,  0.1905, -0.0649,  ..., -0.1006,  0.0018, -0.1040],\n",
       "         [ 0.0225,  0.0873, -0.0699,  ...,  0.0951, -0.0618,  0.0709]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1623,  0.0540,  0.1224,  0.1918,  0.4620, -0.0353,  0.0688,  0.0452,\n",
       "          0.0441,  0.3366,  0.0382,  0.1295,  0.1002,  0.0644, -0.0494, -0.0025,\n",
       "          0.1577,  0.2413, -0.0820, -0.1113,  0.0753,  0.0712,  0.1743,  0.0617,\n",
       "          0.2797, -0.1215,  0.2695,  0.0610,  0.2247,  0.0418,  0.1646, -0.0101,\n",
       "          0.2998,  0.2586,  0.0255,  0.2436,  0.1850,  0.1702,  0.1860, -0.0761,\n",
       "          0.0545,  0.2383,  0.2023, -0.0538,  0.1203,  0.0696, -0.0509, -0.1012,\n",
       "          0.1126,  0.3118,  0.0537,  0.1761, -0.1248, -0.1140,  0.0624,  0.3506,\n",
       "          0.3732,  0.2550,  0.0463,  0.0525,  0.0542,  0.0774,  0.0387, -0.0979,\n",
       "         -0.0589, -0.0746,  0.0221,  0.0827,  0.1230,  0.0102, -0.0670, -0.0069,\n",
       "          0.0235,  0.0693, -0.0141, -0.0871, -0.0835, -0.0010, -0.0557,  0.0175,\n",
       "          0.1937, -0.0997,  0.2264, -0.0063, -0.0901, -0.1870,  0.2266,  0.0740,\n",
       "         -0.1389, -0.0929, -0.0515, -0.1204,  0.0357,  0.1808,  0.0488, -0.0408,\n",
       "          0.0136, -0.0234, -0.1200,  0.0970,  0.2433,  0.0103,  0.0067, -0.0515,\n",
       "          0.2408,  0.0111, -0.0572,  0.0203, -0.0284, -0.0677,  0.0803,  0.0749,\n",
       "         -0.1214,  0.1655, -0.2935,  0.0246, -0.1561,  0.0505, -0.0344,  0.0571,\n",
       "         -0.0511, -0.0294, -0.1874, -0.0282,  0.0444, -0.0607,  0.0577, -0.1368,\n",
       "          0.0254,  0.1197, -0.0164,  0.1055,  0.0468, -0.0984, -0.1131,  0.0104,\n",
       "         -0.0473,  0.0734, -0.0007, -0.0635,  0.0814,  0.0621, -0.0040, -0.1643,\n",
       "          0.1152,  0.0515,  0.0115, -0.0255, -0.0512, -0.2410, -0.0637, -0.1045,\n",
       "         -0.0437, -0.0617,  0.0359,  0.0818, -0.1339, -0.0271, -0.1341, -0.1876,\n",
       "         -0.0177, -0.0799,  0.0615,  0.0433,  0.1026, -0.0135,  0.0812, -0.1460,\n",
       "          0.1237, -0.0203,  0.0949, -0.2257,  0.0031, -0.0626,  0.1435, -0.1100,\n",
       "         -0.0076,  0.0927, -0.0690,  0.0402,  0.0168,  0.1522, -0.0103,  0.1371,\n",
       "         -0.0758, -0.1007, -0.1139, -0.1300,  0.0248,  0.0199,  0.0832, -0.0498],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 2.0548e-01,  1.3079e-01,  1.8063e-01,  3.1601e-01,  3.1554e-01,\n",
       "          1.0879e-01,  7.6811e-02, -6.4157e-02, -2.0594e-02,  1.9669e-01,\n",
       "         -9.9987e-03,  3.5521e-01,  1.0127e-01,  1.4470e-01,  1.0753e-01,\n",
       "         -8.0498e-03,  1.5610e-01,  3.5193e-01, -4.7247e-02, -7.3130e-02,\n",
       "          2.6659e-01, -7.6931e-02,  3.9601e-01,  1.7070e-01,  2.7978e-01,\n",
       "          3.4578e-02,  1.0668e-01,  6.3640e-02,  2.8074e-01,  1.0154e-01,\n",
       "          2.6711e-01,  4.6457e-02,  9.2000e-02,  2.2536e-01,  1.2723e-01,\n",
       "          2.1306e-01,  1.2382e-01,  1.5700e-01,  1.3860e-01, -7.0125e-02,\n",
       "          8.0429e-03,  2.8514e-01,  2.4028e-01, -1.3866e-01,  1.3916e-01,\n",
       "          1.9040e-01,  1.5075e-02,  5.4484e-02, -9.7826e-02,  1.7850e-01,\n",
       "         -1.4451e-01,  1.5293e-01, -1.0247e-02, -1.6048e-01,  1.4366e-01,\n",
       "          1.3135e-01,  1.3941e-01,  2.1422e-01,  2.0186e-01,  1.9844e-01,\n",
       "         -1.2615e-02, -4.8561e-02,  7.2532e-02, -8.4954e-02,  1.3802e-01,\n",
       "          4.9660e-03,  1.6921e-01,  1.0031e-01,  6.2028e-02,  1.2155e-01,\n",
       "          5.4624e-02,  1.4612e-01, -5.6148e-02, -5.4085e-02,  5.7974e-02,\n",
       "          1.0697e-01, -9.9689e-02, -1.4902e-02,  1.3705e-01,  8.2502e-02,\n",
       "          4.4312e-02, -1.4287e-01,  1.8216e-02, -8.8773e-02, -1.2928e-02,\n",
       "          1.9837e-02,  1.0384e-01,  1.4138e-01, -1.3212e-01, -7.2498e-02,\n",
       "          3.4940e-02,  2.2314e-02,  8.6227e-02,  9.1100e-02, -3.5024e-03,\n",
       "          8.1014e-02, -1.4755e-01, -2.4848e-02,  2.8304e-02,  1.2288e-02,\n",
       "          1.2269e-01, -1.4505e-01, -3.9834e-02, -1.1848e-01,  3.6707e-02,\n",
       "         -1.4895e-01,  1.3611e-01,  7.0784e-02, -1.2553e-01, -3.2036e-02,\n",
       "          1.9239e-01, -3.8084e-04, -1.9580e-01,  1.5845e-01, -2.6509e-01,\n",
       "         -5.4349e-02, -1.0792e-01,  1.1266e-01, -1.2863e-01, -1.4437e-01,\n",
       "          6.2113e-03,  6.3096e-02, -5.6411e-02,  5.1744e-02, -7.6234e-03,\n",
       "         -6.5325e-02, -6.5407e-02, -1.6392e-01, -8.8827e-02,  1.4223e-01,\n",
       "          1.3713e-02, -1.1862e-01,  1.1941e-01, -1.3200e-01,  3.7560e-02,\n",
       "          2.1386e-02,  2.3979e-02, -3.9562e-02,  7.8406e-02,  5.5777e-02,\n",
       "          1.0956e-01,  3.3772e-02, -7.8967e-02, -1.8224e-01,  6.1604e-02,\n",
       "         -5.1443e-02,  4.1737e-02, -1.7559e-01,  1.0278e-01, -1.8448e-02,\n",
       "         -5.2624e-02,  3.8930e-02, -2.5419e-02,  1.1792e-01, -7.2175e-03,\n",
       "         -6.2558e-03,  2.9713e-02,  7.3086e-02, -5.7641e-02, -2.9814e-02,\n",
       "         -9.9020e-02, -1.8230e-01, -1.2936e-02,  6.7453e-02,  1.5133e-01,\n",
       "         -8.6301e-02,  5.3724e-02, -9.7473e-03,  6.1901e-02, -1.0155e-01,\n",
       "         -1.1934e-01, -2.0145e-01,  1.5013e-01,  7.5140e-02, -3.8231e-02,\n",
       "         -1.1841e-01,  7.2031e-02,  6.6303e-02, -8.7572e-02,  1.4666e-01,\n",
       "          1.3551e-01,  7.8981e-02, -4.3100e-02, -1.0652e-01,  2.5831e-02,\n",
       "          5.0089e-02, -4.2905e-03, -1.4463e-01,  1.6965e-02, -5.1666e-02,\n",
       "          6.2199e-02,  1.0409e-02], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0994, -0.0763, -0.1170, -0.0735, -0.2624, -0.0790, -0.0300, -0.1314,\n",
       "          -0.0494, -0.1723, -0.0334, -0.0223, -0.1190, -0.0152,  0.0152,  0.0757,\n",
       "          -0.0755, -0.1583,  0.0852, -0.0260,  0.1003, -0.0542, -0.2664, -0.0971,\n",
       "          -0.0511, -0.0366, -0.0710, -0.0254, -0.1930,  0.1220, -0.0970, -0.0569,\n",
       "           0.1214, -0.2849,  0.1287, -0.2234, -0.0986,  0.0250,  0.0120,  0.0652,\n",
       "          -0.0050, -0.2789,  0.0090,  0.0820, -0.0770, -0.0432, -0.0825, -0.0164,\n",
       "          -0.0720,  0.0998, -0.0489, -0.0128,  0.0868,  0.0125,  0.0545, -0.1527,\n",
       "           0.1807, -0.2066,  0.1474,  0.0787,  0.0805, -0.0320, -0.0054,  0.0190],\n",
       "         [ 0.1073, -0.1034,  0.0780,  0.0981,  0.0685,  0.0165, -0.0636, -0.0214,\n",
       "           0.0101,  0.0919, -0.0251,  0.1570,  0.0127, -0.0124, -0.0211,  0.0652,\n",
       "          -0.0529,  0.3054,  0.0658, -0.0383,  0.0781, -0.1573,  0.0260,  0.1293,\n",
       "           0.1123,  0.0250,  0.0129, -0.0215,  0.2783, -0.0365,  0.0145, -0.0323,\n",
       "          -0.1890,  0.1515,  0.0397,  0.1409,  0.0034,  0.0137,  0.0491,  0.0298,\n",
       "          -0.1162,  0.2134, -0.1193,  0.1413,  0.1652,  0.0283, -0.0305,  0.0365,\n",
       "          -0.0235, -0.2580,  0.0384, -0.0549,  0.0929,  0.0883,  0.0578,  0.1402,\n",
       "          -0.1391,  0.4420, -0.0330,  0.1166,  0.0264, -0.0362,  0.0123, -0.0851]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0193, 0.1280], requires_grad=True)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 확인\n",
    "list(simple_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: torch.Size([847, 128]) \n",
      " GRU:  torch.Size([192, 128]) \n",
      " torch.Size([192, 64]) torch.Size([192]) torch.Size([192]) \n",
      " Linear regression :  torch.Size([2, 64]) \n",
      " torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "para = list(simple_model.parameters())\n",
    "print('Embedding:', para[0].shape,'\\n GRU: ', para[1].shape,\n",
    "      '\\n', para[2].shape, para[3].shape, para[4].shape,\n",
    "       '\\n Linear regression : ', para[5].shape,'\\n', para[6].shape)\n",
    "# Model 가중치에는 총 7개 요소가 있으며, 앞서 확인한 \n",
    "# Embedding, RNN, Linear regression 등의 가중치가 모두 포함되었음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([847, 128]) tensor([[ 0.1545, -1.0168, -0.8931,  ..., -0.0169, -1.2699,  0.8142],\n",
      "        [-0.1780,  0.3700,  0.7450,  ..., -0.5840,  0.8755, -0.3447],\n",
      "        [-1.5603, -1.1133,  1.4238,  ...,  1.3853,  0.7623,  0.6323],\n",
      "        ...,\n",
      "        [ 0.1791, -1.3558,  0.7006,  ..., -2.8928,  0.4766,  0.2899],\n",
      "        [ 1.6682,  0.6897, -0.6469,  ..., -0.9862,  1.3039, -0.8010],\n",
      "        [-0.7036,  0.2249,  0.1881,  ...,  0.7784,  0.6586,  0.1922]])\n"
     ]
    }
   ],
   "source": [
    "# 가중치인 generator 동작 확인\n",
    "para = next(simple_model.parameters()).data\n",
    "print(para.shape,para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.4118e-10, 6.4740e-43,        nan,        nan],\n",
       "         [0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43],\n",
       "         [2.2675e-04, 6.4740e-43, 2.2675e-04, 6.4740e-43],\n",
       "         [2.2675e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00],\n",
       "         [2.6905e-43, 0.0000e+00, 3.4119e-10, 6.4740e-43]],\n",
       "\n",
       "        [[3.4119e-10, 6.4740e-43, 3.4119e-10, 6.4740e-43],\n",
       "         [       nan,        nan, 0.0000e+00, 6.4740e-43],\n",
       "         [8.4273e-13, 6.4740e-43, 2.2675e-04, 6.4740e-43],\n",
       "         [2.2675e-04, 6.4740e-43, 2.2675e-04, 6.4740e-43],\n",
       "         [5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00]],\n",
       "\n",
       "        [[3.4121e-10, 6.4740e-43, 3.4121e-10, 6.4740e-43],\n",
       "         [3.4121e-10, 6.4740e-43,        nan,        nan],\n",
       "         [0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43],\n",
       "         [2.2675e-04, 6.4740e-43, 2.2675e-04, 6.4740e-43],\n",
       "         [2.2675e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00]],\n",
       "\n",
       "        [[2.6905e-43, 0.0000e+00, 3.4123e-10, 6.4740e-43],\n",
       "         [3.4123e-10, 6.4740e-43, 3.4123e-10, 6.4740e-43],\n",
       "         [       nan,        nan, 0.0000e+00, 6.4740e-43],\n",
       "         [8.4273e-13, 6.4740e-43, 2.2675e-04, 6.4740e-43],\n",
       "         [2.2675e-04, 6.4740e-43, 2.2675e-04, 6.4740e-43]],\n",
       "\n",
       "        [[5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00],\n",
       "         [3.4125e-10, 6.4740e-43, 3.4125e-10, 6.4740e-43],\n",
       "         [3.4125e-10, 6.4740e-43,        nan,        nan],\n",
       "         [0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43],\n",
       "         [2.2676e-04, 6.4740e-43, 2.2676e-04, 6.4740e-43]],\n",
       "\n",
       "        [[2.2676e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00],\n",
       "         [2.6905e-43, 0.0000e+00, 3.4127e-10, 6.4740e-43],\n",
       "         [3.4127e-10, 6.4740e-43, 3.4127e-10, 6.4740e-43],\n",
       "         [       nan,        nan, 0.0000e+00, 6.4740e-43],\n",
       "         [8.4273e-13, 6.4740e-43, 2.2676e-04, 6.4740e-43]],\n",
       "\n",
       "        [[2.2676e-04, 6.4740e-43, 2.2676e-04, 6.4740e-43],\n",
       "         [5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00],\n",
       "         [3.4129e-10, 6.4740e-43, 3.4129e-10, 6.4740e-43],\n",
       "         [3.4129e-10, 6.4740e-43,        nan,        nan],\n",
       "         [0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43]],\n",
       "\n",
       "        [[2.2676e-04, 6.4740e-43, 2.2676e-04, 6.4740e-43],\n",
       "         [2.2676e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00],\n",
       "         [2.6905e-43, 0.0000e+00, 3.4131e-10, 6.4740e-43],\n",
       "         [3.4131e-10, 6.4740e-43, 3.4131e-10, 6.4740e-43],\n",
       "         [       nan,        nan, 0.0000e+00, 6.4740e-43]],\n",
       "\n",
       "        [[8.4273e-13, 6.4740e-43, 2.2676e-04, 6.4740e-43],\n",
       "         [2.2676e-04, 6.4740e-43, 2.2676e-04, 6.4740e-43],\n",
       "         [5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00],\n",
       "         [3.4133e-10, 6.4740e-43, 3.4133e-10, 6.4740e-43],\n",
       "         [3.4133e-10, 6.4740e-43,        nan,        nan]],\n",
       "\n",
       "        [[0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43],\n",
       "         [2.2676e-04, 6.4740e-43, 2.2676e-04, 6.4740e-43],\n",
       "         [2.2676e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00],\n",
       "         [2.6905e-43, 0.0000e+00, 3.4135e-10, 6.4740e-43],\n",
       "         [3.4135e-10, 6.4740e-43, 3.4135e-10, 6.4740e-43]],\n",
       "\n",
       "        [[       nan,        nan, 0.0000e+00, 6.4740e-43],\n",
       "         [8.4273e-13, 6.4740e-43, 2.2676e-04, 6.4740e-43],\n",
       "         [2.2676e-04, 6.4740e-43, 2.2676e-04, 6.4740e-43],\n",
       "         [5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00],\n",
       "         [3.4137e-10, 6.4740e-43, 3.4137e-10, 6.4740e-43]],\n",
       "\n",
       "        [[3.4137e-10, 6.4740e-43,        nan,        nan],\n",
       "         [0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43],\n",
       "         [2.2676e-04, 6.4740e-43, 2.2676e-04, 6.4740e-43],\n",
       "         [2.2676e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00],\n",
       "         [2.6905e-43, 0.0000e+00, 3.4139e-10, 6.4740e-43]],\n",
       "\n",
       "        [[3.4139e-10, 6.4740e-43, 3.4139e-10, 6.4740e-43],\n",
       "         [       nan,        nan, 0.0000e+00, 6.4740e-43],\n",
       "         [8.4273e-13, 6.4740e-43, 2.2676e-04, 6.4740e-43],\n",
       "         [2.2676e-04, 6.4740e-43, 2.2676e-04, 6.4740e-43],\n",
       "         [5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00]],\n",
       "\n",
       "        [[3.4141e-10, 6.4740e-43, 3.4141e-10, 6.4740e-43],\n",
       "         [3.4141e-10, 6.4740e-43,        nan,        nan],\n",
       "         [0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43],\n",
       "         [2.2677e-04, 6.4740e-43, 2.2677e-04, 6.4740e-43],\n",
       "         [2.2677e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00]],\n",
       "\n",
       "        [[2.6905e-43, 0.0000e+00, 3.4143e-10, 6.4740e-43],\n",
       "         [3.4143e-10, 6.4740e-43, 3.4143e-10, 6.4740e-43],\n",
       "         [       nan,        nan, 0.0000e+00, 6.4740e-43],\n",
       "         [8.4273e-13, 6.4740e-43, 2.2677e-04, 6.4740e-43],\n",
       "         [2.2677e-04, 6.4740e-43, 2.2677e-04, 6.4740e-43]],\n",
       "\n",
       "        [[5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00],\n",
       "         [3.4145e-10, 6.4740e-43, 3.4145e-10, 6.4740e-43],\n",
       "         [3.4145e-10, 6.4740e-43,        nan,        nan],\n",
       "         [0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43],\n",
       "         [2.2677e-04, 6.4740e-43, 2.2677e-04, 6.4740e-43]],\n",
       "\n",
       "        [[2.2677e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00],\n",
       "         [2.6905e-43, 0.0000e+00, 3.4147e-10, 6.4740e-43],\n",
       "         [3.4147e-10, 6.4740e-43, 3.4147e-10, 6.4740e-43],\n",
       "         [       nan,        nan, 0.0000e+00, 6.4740e-43],\n",
       "         [8.4273e-13, 6.4740e-43, 2.2677e-04, 6.4740e-43]],\n",
       "\n",
       "        [[2.2677e-04, 6.4740e-43, 2.2677e-04, 6.4740e-43],\n",
       "         [5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00],\n",
       "         [3.4149e-10, 6.4740e-43, 3.4149e-10, 6.4740e-43],\n",
       "         [3.4149e-10, 6.4740e-43,        nan,        nan],\n",
       "         [0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43]],\n",
       "\n",
       "        [[2.2677e-04, 6.4740e-43, 2.2677e-04, 6.4740e-43],\n",
       "         [2.2677e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00],\n",
       "         [2.6905e-43, 0.0000e+00, 3.4151e-10, 6.4740e-43],\n",
       "         [3.4151e-10, 6.4740e-43, 3.4151e-10, 6.4740e-43],\n",
       "         [       nan,        nan, 0.0000e+00, 6.4740e-43]],\n",
       "\n",
       "        [[8.4273e-13, 6.4740e-43, 2.2677e-04, 6.4740e-43],\n",
       "         [2.2677e-04, 6.4740e-43, 2.2677e-04, 6.4740e-43],\n",
       "         [5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00],\n",
       "         [3.4153e-10, 6.4740e-43, 3.4153e-10, 6.4740e-43],\n",
       "         [3.4153e-10, 6.4740e-43,        nan,        nan]],\n",
       "\n",
       "        [[0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43],\n",
       "         [2.2677e-04, 6.4740e-43, 2.2677e-04, 6.4740e-43],\n",
       "         [2.2677e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00],\n",
       "         [2.6905e-43, 0.0000e+00, 3.4155e-10, 6.4740e-43],\n",
       "         [3.4155e-10, 6.4740e-43, 3.4155e-10, 6.4740e-43]],\n",
       "\n",
       "        [[       nan,        nan, 0.0000e+00, 6.4740e-43],\n",
       "         [8.4273e-13, 6.4740e-43, 2.2677e-04, 6.4740e-43],\n",
       "         [2.2677e-04, 6.4740e-43, 2.2677e-04, 6.4740e-43],\n",
       "         [5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00],\n",
       "         [3.4157e-10, 6.4740e-43, 3.4157e-10, 6.4740e-43]],\n",
       "\n",
       "        [[3.4157e-10, 6.4740e-43,        nan,        nan],\n",
       "         [0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43],\n",
       "         [2.2678e-04, 6.4740e-43, 2.2678e-04, 6.4740e-43],\n",
       "         [2.2678e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00],\n",
       "         [2.6905e-43, 0.0000e+00, 3.4159e-10, 6.4740e-43]],\n",
       "\n",
       "        [[3.4159e-10, 6.4740e-43, 3.4159e-10, 6.4740e-43],\n",
       "         [       nan,        nan, 0.0000e+00, 6.4740e-43],\n",
       "         [8.4273e-13, 6.4740e-43, 2.2678e-04, 6.4740e-43],\n",
       "         [2.2678e-04, 6.4740e-43, 2.2678e-04, 6.4740e-43],\n",
       "         [5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00]],\n",
       "\n",
       "        [[3.4161e-10, 6.4740e-43, 3.4161e-10, 6.4740e-43],\n",
       "         [3.4161e-10, 6.4740e-43,        nan,        nan],\n",
       "         [0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43],\n",
       "         [2.2678e-04, 6.4740e-43, 2.2678e-04, 6.4740e-43],\n",
       "         [2.2678e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00]],\n",
       "\n",
       "        [[2.6905e-43, 0.0000e+00, 3.4162e-10, 6.4740e-43],\n",
       "         [3.4163e-10, 6.4740e-43, 3.4163e-10, 6.4740e-43],\n",
       "         [       nan,        nan, 0.0000e+00, 6.4740e-43],\n",
       "         [8.4273e-13, 6.4740e-43, 2.2678e-04, 6.4740e-43],\n",
       "         [2.2678e-04, 6.4740e-43, 2.2678e-04, 6.4740e-43]],\n",
       "\n",
       "        [[5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00],\n",
       "         [3.4165e-10, 6.4740e-43, 3.4165e-10, 6.4740e-43],\n",
       "         [3.4165e-10, 6.4740e-43,        nan,        nan],\n",
       "         [0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43],\n",
       "         [2.2678e-04, 6.4740e-43, 2.2678e-04, 6.4740e-43]],\n",
       "\n",
       "        [[2.2678e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00],\n",
       "         [2.6905e-43, 0.0000e+00, 3.4167e-10, 6.4740e-43],\n",
       "         [3.4167e-10, 6.4740e-43, 3.4167e-10, 6.4740e-43],\n",
       "         [       nan,        nan, 0.0000e+00, 6.4740e-43],\n",
       "         [8.4273e-13, 6.4740e-43, 2.2678e-04, 6.4740e-43]],\n",
       "\n",
       "        [[2.2678e-04, 6.4740e-43, 2.2678e-04, 6.4740e-43],\n",
       "         [5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00],\n",
       "         [3.4169e-10, 6.4740e-43, 3.4169e-10, 6.4740e-43],\n",
       "         [3.4169e-10, 6.4740e-43,        nan,        nan],\n",
       "         [0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43]],\n",
       "\n",
       "        [[2.2678e-04, 6.4740e-43, 2.2678e-04, 6.4740e-43],\n",
       "         [2.2678e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00],\n",
       "         [2.6905e-43, 0.0000e+00, 3.4032e-10, 6.4740e-43],\n",
       "         [3.4032e-10, 6.4740e-43, 3.4033e-10, 6.4740e-43],\n",
       "         [       nan,        nan, 0.0000e+00, 6.4740e-43]],\n",
       "\n",
       "        [[8.4273e-13, 6.4740e-43, 2.2678e-04, 6.4740e-43],\n",
       "         [2.2678e-04, 6.4740e-43, 2.2679e-04, 6.4740e-43],\n",
       "         [5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00],\n",
       "         [8.9249e-11, 6.4740e-43, 8.9249e-11, 6.4740e-43],\n",
       "         [8.9249e-11, 6.4740e-43,        nan,        nan]],\n",
       "\n",
       "        [[0.0000e+00, 6.4740e-43, 8.4273e-13, 6.4740e-43],\n",
       "         [2.2679e-04, 6.4740e-43, 2.2679e-04, 6.4740e-43],\n",
       "         [2.2679e-04, 6.4740e-43, 5.6052e-45, 0.0000e+00],\n",
       "         [2.6905e-43, 0.0000e+00, 8.9249e-11, 6.4740e-43],\n",
       "         [8.9249e-11, 6.4740e-43, 8.9249e-11, 6.4740e-43]],\n",
       "\n",
       "        [[       nan,        nan, 0.0000e+00, 6.4740e-43],\n",
       "         [8.4273e-13, 6.4740e-43, 2.2679e-04, 6.4740e-43],\n",
       "         [2.2679e-04, 6.4740e-43, 2.2679e-04, 6.4740e-43],\n",
       "         [5.6052e-45, 0.0000e+00, 2.6905e-43, 0.0000e+00],\n",
       "         [8.9249e-11, 6.4740e-43, 8.9249e-11, 6.4740e-43]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 차원 변형 방법\n",
    "para.new(33, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 초기화 방법\n",
    "para.new(32, 5).zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
