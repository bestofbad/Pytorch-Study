{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로지스틱 회귀(Logistic Regression)\n",
    "\n",
    "- 이진 분류(Binary Classification)\n",
    "\n",
    ": 시험 점수가 합격인지 불합격인지, 어떤 메일을 받았을 때 이게 정상 메일인지 스팸 메일인지를 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H(x)=sigmoid(Wx+b)=1/(1+e −(Wx+b)) =σ(Wx+b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost(H(x),y)=−[ylogH(x)+(1−y)log(1−H(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 함수 결과 의미\n",
    "여러번의 퀴즈 결과로 시험 점수를 예측했을 때, 이를 다시 시험 합격/불합격으로 예측한다면,\n",
    "1. y = Wx+b : 각 퀴즈 결과 (x 입력 변수)로 시험 점수(y)의 예상치를 계산한다.\n",
    "2. y (시험 점수)는 Sigmoid 함수에서 x 값이 되며, x 축의 '0' 주변쯤 어딘가 위치한다.\n",
    "3. y (시험 점수)는 x축 우측으로 갈 수록 Sigmoid 계산 결과가 1에 가까워진다.\n",
    "4. y가 입력된 Sigmoid 결과는 0과 1사이의 값을 가지며, \n",
    "   어떤 기준(예에서는 0.5쯤)을 넘으면 1에 가깝다, 그러니 True 다... 이는 시험 합격이다... 이렇게 판단한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f362c326f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling 방법\n",
    "\n",
    "1. 고전적으로 함수를 일일히 정의\n",
    "\n",
    "2. nn.Module을 써서 함수를 간략히 정의\n",
    "\n",
    "3. class 구조까지 사용하여 정의\n",
    "\n",
    "** 아래에서 1, 2, 3 방법 중 한 가지만 실행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-1 가중치 w 와 편향 b 를 Manual로 선언하는 방법\n",
    "# Variable method로 w와 b에 aurograd 가능함을 선언\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# numpy array로 W, b 정의\n",
    "W = np.zeros((2, 1))\n",
    "b = np.zeros(1)\n",
    "\n",
    "# numpy를 torch Tensor 변환 및 aurograd 전환\n",
    "W = torch.Tensor(W)\n",
    "b = torch.Tensor(b)\n",
    "\n",
    "W = torch.autograd.Variable(W, requires_grad=True)\n",
    "b = torch.autograd.Variable(b, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2 numpy없이, 가중치 w 와 편향 b 를 torch Tensor 및 aurograd option으로 선언\n",
    "W = torch.zeros((2,1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.693147\n",
      "Epoch  100/1000 Cost: 0.134722\n",
      "Epoch  200/1000 Cost: 0.080643\n",
      "Epoch  300/1000 Cost: 0.057900\n",
      "Epoch  400/1000 Cost: 0.045300\n",
      "Epoch  500/1000 Cost: 0.037261\n",
      "Epoch  600/1000 Cost: 0.031672\n",
      "Epoch  700/1000 Cost: 0.027556\n",
      "Epoch  800/1000 Cost: 0.024394\n",
      "Epoch  900/1000 Cost: 0.021888\n",
      "Epoch 1000/1000 Cost: 0.019852\n"
     ]
    }
   ],
   "source": [
    "#1 \n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    hypothesis = 1/ (1+torch.exp(-(x_train.matmul(W)+b)))\n",
    "    # 또는 sigmoid 내장함수 사용\n",
    "    # hypothesis = torch.sigmoid(x_train.matmul(W)+b)\n",
    "\n",
    "    losses = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis))\n",
    "    cost = losses.mean()  # 전체 오파에 대한 평균\n",
    "    # 또는 cross entropy 내장함수 사용\n",
    "    # cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "model = nn.Sequential(nn.Linear(2,1),     # nn.Linear에 '2'는 x값 2차원, 이에 맞춰 W와 b가 랜덤 초기화 됨\n",
    "                      nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))     # nn.Sequential함수대신 Sigmoid 직접 입력\n",
    "\n",
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.539713 Accuracy 83.33%\n",
      "Epoch  100/1000 Cost: 0.134272 Accuracy 100.00%\n",
      "Epoch  200/1000 Cost: 0.080486 Accuracy 100.00%\n",
      "Epoch  300/1000 Cost: 0.057820 Accuracy 100.00%\n",
      "Epoch  400/1000 Cost: 0.045251 Accuracy 100.00%\n",
      "Epoch  500/1000 Cost: 0.037228 Accuracy 100.00%\n",
      "Epoch  600/1000 Cost: 0.031649 Accuracy 100.00%\n",
      "Epoch  700/1000 Cost: 0.027538 Accuracy 100.00%\n",
      "Epoch  800/1000 Cost: 0.024381 Accuracy 100.00%\n",
      "Epoch  900/1000 Cost: 0.021877 Accuracy 100.00%\n",
      "Epoch 1000/1000 Cost: 0.019843 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "#2, 3 공통\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    hypothesis = model(x_train)\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "      # 예측값이 0.5를 넘으면 True로 간주하며 해석 논리는 아래 '#1'의 학습결과 확인 내용 참조\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "      # True,False 값인 prediction을 1과 0인 값으로 바꾸고, y_train 실제값들과 비교하며, 일치하면 True로 간주\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "      # 정확도: correct_prediction은 1 또는 0을 갖는 행렬이므로, 행렬 원소 전체 합을 행렬크기로 나눔\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 학습 결과 확인\n",
    "\n",
    "현재 W와 b는 훈련 후의 값을 가지고 있으며, 학습된 W와 b 및 예측값을 출력해 봄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7648e-04],\n",
      "        [3.1608e-02],\n",
      "        [3.8977e-02],\n",
      "        [9.5622e-01],\n",
      "        [9.9823e-01],\n",
      "        [9.9969e-01]], grad_fn=<SigmoidBackward>) \n",
      " tensor([[3.2530],\n",
      "        [1.5179]], requires_grad=True) \n",
      " tensor([-14.4819], requires_grad=True)\n",
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True]])\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "print(hypothesis, \"\\n\", W,\"\\n\", b)\n",
    "\n",
    "#이제 0.5를 넘으면 True, 넘지 않으면 False로 값을 정하여 출력.\n",
    "\n",
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[3.2534, 1.5181]], requires_grad=True), Parameter containing:\n",
      "tensor([-14.4839], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "#2, 3 공통 :  훈련 후의 W와 b\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reference \n",
    "\n",
    " -. https://wikidocs.net/57810\n",
    " \n",
    " -. https://www.youtube.com/watch?v=Mf8jna42p2M&list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m&index=7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
