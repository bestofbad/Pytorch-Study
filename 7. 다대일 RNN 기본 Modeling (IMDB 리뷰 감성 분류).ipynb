{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 다대일 RNN 기본 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu와 cuda 중 다음 기기로 학습함: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2585c3f46d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets\n",
    "import random\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", DEVICE)\n",
    "\n",
    "SEED = 5\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-1. Review Data 수집 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('IMDb_Reviews.csv', <http.client.HTTPMessage at 0x28a2c456d08>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수 : 50000\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 개수 : {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It would be unfair to the actors to condemn th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  It would be unfair to the actors to condemn th...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample 수가 많으므로, Test용으로 대폭 줄여 새로운 파일을 만든 후 사용\n",
    "df = pd.read_csv('IMDb_Reviews_test.csv', encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수 : 20\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 개수 : {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 데이터, 평가 데이터, 테스트 데이터로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[:15]\n",
    "test_df = df[15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_data.csv\", index=False)\n",
    "test_df.to_csv(\"test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-2. Field 정의 및 Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True,lower=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = TabularDataset.splits(\n",
    "        path='.', train='train_data.csv', test='test_data.csv', format='csv',\n",
    "        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset sample 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 개수 : 15\n",
      "테스트 샘플의 개수 : 5\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플의 개수 : {}'.format(len(trainset)))\n",
    "print('테스트 샘플의 개수 : {}'.format(len(testset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x000002586019D508>\n"
     ]
    }
   ],
   "source": [
    "print(trainset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['in', 'truth', 'though,', \"there's\", 'a', 'lack', 'of', 'dramatic', 'tension', 'throughout', 'for', 'which', 'the', 'action', 'sequences', \"don't\", 'fully', 'compensate', 'and', 'you', \"don't\", 'care', 'a', 'fig', 'for', 'any', 'of', 'the', 'leading', 'characters.', 'one', 'of', 'those', 'films', 'where', 'the', 'actors', 'probably', 'enjoyed', 'making', 'it', 'more', 'than', 'the', 'viewers', 'did', 'watching', 'it.'], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "# trainset 내용물 확인\n",
    "print(vars(testset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-3. Vocabulary set 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trainset, min_freq=3)          # 단어 집합 생성, 단어 수가 적으므로 최소 사용 횟수를 3으로 설정\n",
    "LABEL.build_vocab(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 33\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(TEXT.vocab)\n",
    "print('단어 집합의 크기 : {}'.format(n_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x00000258601BB6C8>>, {'<unk>': 0, '<pad>': 1, 'the': 2, 'of': 3, 'to': 4, 'and': 5, 'a': 6, 'it': 7, 'i': 8, 'one': 9, 'is': 10, 'this': 11, 'was': 12, 'in': 13, 'that': 14, 'but': 15, 'ever': 16, 'you': 17, 'are': 18, 'for': 19, 'movie': 20, 'be': 21, 'by': 22, 'first': 23, 'had': 24, \"it's\": 25, 'just': 26, 'make': 27, 'most': 28, 'movies': 29, 'not': 30, 'there': 31, 'they': 32})\n"
     ]
    }
   ],
   "source": [
    "# 수집된 전체 단어 확인\n",
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_2, valset = trainset.split(split_ratio=0.45)       # 훈련 데이터와 평가 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 개수 : 7\n",
      "평가 샘플의 개수 : 8\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플의 개수 : {}'.format(len(trainset_2)))\n",
    "print('평가 샘플의 개수 : {}'.format(len(valset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-4. Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "        (trainset_2, valset, testset), batch_size=BATCH_SIZE,\n",
    "        shuffle=True, repeat=False, sort=False)                            # shuffle 진행 안하면 Data 변환 추적 쉬움."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치의 개수 : 4\n",
      "검증 데이터의 미니 배치의 개수 : 4\n",
      "테스트 데이터의 미니 배치의 개수 : 3\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 미니 배치의 개수 : {}'.format(len(train_iter)))\n",
    "print('검증 데이터의 미니 배치의 개수 : {}'.format(len(val_iter)))\n",
    "print('테스트 데이터의 미니 배치의 개수 : {}'.format(len(test_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 2]\n",
      "\t[.text]:[torch.LongTensor of size 2x53]\n",
      "\t[.label]:[torch.LongTensor of size 2]\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))       # Dataloader가 iterator 역할을 잘하는지 확인\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 2]\n",
      "\t[.text]:[torch.LongTensor of size 2x53]\n",
      "\t[.label]:[torch.LongTensor of size 2]\n"
     ]
    }
   ],
   "source": [
    "batch2 = next(iter(val_iter))      # data.BucketIterator.splits 조건에 sort=False를 하지 않으면  \" '<' not supported ... \" Error 발생\n",
    "print(batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 샘플의 개수 재확인 : 7\n",
      "검증 데이터의 샘플의 개수 재확인 : 8\n",
      "테스트 데이터의 샘플의 개수 재확인 : 5\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 샘플의 개수 재확인 : {}'.format(len(train_iter.dataset)))\n",
    "print('검증 데이터의 샘플의 개수 재확인 : {}'.format(len(val_iter.dataset)))\n",
    "print('테스트 데이터의 샘플의 개수 재확인 : {}'.format(len(test_iter.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch Iterator test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b is 0. batch is \n",
      "[torchtext.data.batch.Batch of size 2]\n",
      "\t[.text]:[torch.LongTensor of size 2x31]\n",
      "\t[.label]:[torch.LongTensor of size 2]\n",
      "b is 1. batch is \n",
      "[torchtext.data.batch.Batch of size 2]\n",
      "\t[.text]:[torch.LongTensor of size 2x53]\n",
      "\t[.label]:[torch.LongTensor of size 2]\n",
      "b is 2. batch is \n",
      "[torchtext.data.batch.Batch of size 2]\n",
      "\t[.text]:[torch.LongTensor of size 2x49]\n",
      "\t[.label]:[torch.LongTensor of size 2]\n",
      "b is 3. batch is \n",
      "[torchtext.data.batch.Batch of size 1]\n",
      "\t[.text]:[torch.LongTensor of size 1x44]\n",
      "\t[.label]:[torch.LongTensor of size 1]\n"
     ]
    }
   ],
   "source": [
    "for b, batch in enumerate(train_iter):\n",
    "    x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "    print('b is {}. batch is {}'. format(b, batch)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is tensor([[ 2,  9,  0, 10,  2,  0,  4,  0, 13,  0,  0,  0,  0,  0,  0,  0,  4,  0,\n",
      "          0, 13,  2,  0,  0, 11, 26,  0,  2,  0,  6,  0,  0,  0, 15,  0, 17,  0,\n",
      "          0,  0,  0, 17,  0,  0,  6,  0,  0,  4,  0,  0,  0],\n",
      "        [31, 10,  0,  0, 14,  0, 10, 22,  0,  9,  3,  2,  0,  0, 16, 30,  0, 13,\n",
      "          0,  0, 15,  0,  0,  0,  2,  0,  0,  0,  0,  2,  0,  7,  0,  0,  0,  0,\n",
      "          3,  0,  0,  7,  0,  4,  0,  0,  1,  1,  1,  1,  1]]). y is tensor([1, 1])\n",
      "tensor([0, 0])\n",
      "x is tensor([[ 0,  0,  5,  8,  0,  0, 30,  0,  0, 29, 19,  2,  0,  0, 14, 32, 18,  0,\n",
      "          0, 32,  0,  2,  0,  5, 26, 30,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 7, 10,  0,  5,  8,  0,  7,  4,  0, 17,  0,  0,  0,  0,  5,  0,  5,  0,\n",
      "          0,  0, 19,  0,  5,  0,  3,  0,  0,  4,  0,  8,  0,  7,  0,  0,  5,  0,\n",
      "          8,  0,  4,  0,  0,  0,  0]]). y is tensor([1, 1])\n",
      "tensor([0, 0])\n",
      "x is tensor([[ 8,  0,  2,  0,  3, 11,  5,  8, 12,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
      "          0, 31, 18,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1],\n",
      "        [ 2,  0,  0,  0, 13,  0,  0,  5,  0, 21,  0,  0, 13, 25,  0,  0,  0,  0,\n",
      "         17,  0, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]). y is tensor([1, 2])\n",
      "tensor([0, 1])\n",
      "x is tensor([[ 7, 12,  2,  0,  5,  0,  0,  0,  0,  5,  6,  0,  0,  0,  0,  0,  0,  4,\n",
      "         27,  6,  0,  0,  0,  0,  0,  0,  3, 11,  0,  0, 12,  0,  4,  0,  2,  0,\n",
      "          0, 10,  9,  3,  2, 28,  0,  0,  0, 16,  0,  6,  0,  4, 27,  6,  0]]). y is tensor([2])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "for b, batch in enumerate(train_iter):\n",
    "    x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "    print('x is {}. y is {}'. format(x, y)) \n",
    "    print(y.data.sub_(1))      # lable 값 조정: y값에서 () 값을 뺌, sub 대신 add를 하면 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7, 12,  2,  0,  5,  0,  0,  0,  0,  5,  6,  0,  0,  0,  0,  0,  0,  4,\n",
       "         27,  6,  0,  0,  0,  0,  0,  0,  3, 11,  0,  0, 12,  0,  4,  0,  2,  0,\n",
       "          0, 10,  9,  3,  2, 28,  0,  0,  0, 16,  0,  6,  0,  4, 27,  6,  0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " x            # 현재 x 입력값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 53])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " x.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-5. Reviewing RNN Model\n",
    "\n",
    "* Embedding, RNN, Cost Function 동작 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2   # 분류되어야 할 결과 수 (긍정 or 부정)\n",
    "embed_dim= 5  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n",
    "hidden_size = 20  # RNN의 은닉층 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.8423e+00,  5.1889e-01, -1.7119e+00, -1.7014e+00,  2.0194e+00],\n",
      "        [-2.6861e-01, -1.3072e-01, -1.4374e+00,  3.9077e-01, -1.8968e-02],\n",
      "        [-1.3527e+00, -7.3082e-01,  9.8792e-01, -4.1941e-01, -5.8490e-01],\n",
      "        [-7.8233e-01,  2.7799e+00,  1.2220e+00, -3.3645e-01, -9.6506e-01],\n",
      "        [-1.2966e-01, -6.0177e-01,  1.4500e-01, -1.4983e-01, -4.3740e-01],\n",
      "        [ 7.7923e-01, -5.8339e-02, -2.0305e+00,  1.4829e+00,  4.9404e-01],\n",
      "        [ 2.4922e-01,  1.7470e+00, -2.6167e-01, -7.3239e-01,  1.6980e+00],\n",
      "        [-1.7917e-01,  1.9231e+00,  2.8795e-01,  9.3680e-01, -2.4031e+00],\n",
      "        [-1.4789e-01,  8.9670e-01,  5.4813e-01, -1.6391e+00, -1.8153e+00],\n",
      "        [-2.0663e-01, -5.2595e-01, -1.6977e+00,  8.1679e-01,  4.0964e-01],\n",
      "        [ 2.1623e-01,  1.0898e+00,  1.7303e-01,  1.6677e-01, -1.1372e+00],\n",
      "        [-8.2083e-01, -1.7927e+00,  8.7719e-01,  2.1641e+00, -1.8553e-01],\n",
      "        [ 9.9064e-01,  7.4259e-01,  1.2702e+00,  1.3227e+00, -5.1811e-01],\n",
      "        [ 1.5633e+00,  1.2358e+00,  1.4962e+00,  4.7312e-01,  6.0702e-01],\n",
      "        [-8.7793e-01, -7.4259e-01,  2.8316e-01, -1.4981e+00, -3.0913e-01],\n",
      "        [-6.0410e-01, -1.9672e+00, -1.3676e-01,  1.8703e+00,  1.5050e-01],\n",
      "        [-8.1565e-01, -3.8387e-01,  4.0206e-01, -1.4338e+00, -7.2579e-02],\n",
      "        [-5.4705e-01,  4.8782e-01, -6.5195e-01, -3.6421e-01,  1.0474e+00],\n",
      "        [ 9.9757e-01,  9.2672e-01,  1.9252e+00, -4.7434e-02,  5.1863e-01],\n",
      "        [ 6.5188e-01, -8.3233e-01, -1.8556e-01,  1.2361e+00, -1.4389e+00],\n",
      "        [-8.9084e-01, -2.5974e-01, -2.3202e-01,  4.4492e-01,  2.5572e-01],\n",
      "        [ 1.0455e+00,  1.7144e+00, -1.2035e+00, -1.8259e-01,  1.3226e-01],\n",
      "        [ 5.0329e-01, -2.2264e-01, -1.1186e-01, -2.1019e-01,  9.2921e-01],\n",
      "        [-7.4944e-01,  4.4238e-02,  1.0023e+00, -1.5020e+00, -1.3211e+00],\n",
      "        [-1.5640e-01,  1.1380e+00, -1.3308e+00,  1.8092e+00,  3.6398e-01],\n",
      "        [-3.0125e-01, -1.6497e+00,  2.8655e-02,  3.8647e-01,  9.6968e-01],\n",
      "        [ 1.9006e-01,  1.8483e-01,  8.8378e-01,  1.3211e+00, -1.3169e-01],\n",
      "        [-1.4731e+00, -4.2345e-01,  2.1471e-01, -4.5853e-01,  5.6842e-01],\n",
      "        [ 5.8099e-01, -2.6558e-01,  4.1469e-01,  2.5220e+00, -1.2157e+00],\n",
      "        [-1.7087e-02, -4.8851e-01,  9.5343e-01, -8.8934e-01, -1.1835e+00],\n",
      "        [ 1.2763e+00,  8.3973e-01,  1.4283e+00,  1.9547e+00,  2.8816e-01],\n",
      "        [-1.7719e+00,  1.2488e+00,  5.5488e-01,  3.4823e-01,  1.6954e-03],\n",
      "        [ 1.1403e+00,  1.1029e+00,  4.9841e-01,  2.3024e+00,  7.3816e-01]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# num_embedding는 trainset 단어 전체 갯수인 n_vocab로 지정\n",
    "Emb_Test=nn.Embedding(num_embeddings=n_vocab,   embedding_dim=embed_dim)\n",
    "print(Emb_Test.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 53, 5])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emb_Test(x).shape           # 임베딩 결과 차원 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN input Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_test = nn.RNN(embed_dim, hidden_size, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 53, 20])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, hidden = rnn_test (Emb_Test(x))      # Tuple 형태의 결과를 분리\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3476,  0.3406,  0.4418,  ..., -0.8358,  0.5103, -0.3813],\n",
       "         [-0.8339,  0.4391,  0.2956,  ..., -0.4438,  0.1044, -0.2850],\n",
       "         [-0.6092,  0.1649,  0.4299,  ..., -0.4201, -0.5689, -0.5137],\n",
       "         ...,\n",
       "         [-0.0237,  0.0577,  0.2442,  ..., -0.1295, -0.5892, -0.1148],\n",
       "         [-0.3771,  0.3087,  0.6000,  ..., -0.1882, -0.3551, -0.2562],\n",
       "         [-0.2805, -0.0598,  0.3699,  ...,  0.2670, -0.7350, -0.0822]]],\n",
       "       grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return Analysis\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "* 리뷰 감성 분류는 긍정/부정 하나의 분류이므로, RNN 다대일 구조이며, 이 경우 RNN 연산 결과는 n개 은닉 상태 중에서 마지막 번째만 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2805, -0.0598,  0.3699, -0.0104, -0.4839,  0.8506,  0.1663,  0.6903,\n",
       "         -0.0231, -0.6711, -0.5414,  0.0972, -0.3279, -0.1136, -0.8845, -0.5998,\n",
       "          0.6288,  0.2670, -0.7350, -0.0822]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression test for Binary Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0819, -0.0214]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_test = nn.Linear(hidden_size, n_classes)\n",
    "linear_test(output[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7461, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = linear_test(output[:, -1, :])\n",
    "F.cross_entropy(out, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-6. Designing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Modeling\n",
    "\n",
    "* Embedding -> RNN -> binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel_1(nn.Module):\n",
    "    def __init__(self, hidden_size, n_vocab, embed_dim, n_classes, batch_first=True):    \n",
    "        super(myModel_1, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=n_vocab, embedding_dim=embed_dim)\n",
    "        self.rnn_layer = nn.RNN(embed_dim, hidden_size, batch_first=batch_first)\n",
    "        self.linear = nn.Linear(hidden_size, n_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.embedding_layer(x)\n",
    "        output, hidden = self.rnn_layer(output)\n",
    "        output = self.linear(output[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "simple_model = myModel_1(hidden_size, n_vocab, embed_dim, n_classes, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3953, -0.0669]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model(x)           # Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=simple_model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/200] output is tensor([[ 0.0869, -0.6773],\n",
      "        [ 0.2227, -0.7069]]), y is tensor([0, 0]), and loss is 0.7150 \n",
      "[40/200] output is tensor([[-0.6659,  0.2290]]), y is tensor([1]), and loss is 0.3426 \n",
      "[60/200] output is tensor([[-1.1810,  0.7455],\n",
      "        [ 0.5542, -1.1610]]), y is tensor([1, 1]), and loss is 2.0166 \n",
      "[80/200] output is tensor([[ 0.5643, -1.1482],\n",
      "        [ 0.5500, -1.0792]]), y is tensor([1, 0]), and loss is 2.0574 \n",
      "[100/200] output is tensor([[ 1.0069, -1.4312],\n",
      "        [ 1.0902, -1.7369]]), y is tensor([0, 0]), and loss is 0.1412 \n",
      "[120/200] output is tensor([[ 0.8377, -1.4073],\n",
      "        [ 1.1594, -1.6013]]), y is tensor([0, 0]), and loss is 0.1620 \n",
      "[140/200] output is tensor([[ 0.4676, -1.0658],\n",
      "        [ 1.6302, -2.1382]]), y is tensor([0, 0]), and loss is 0.2182 \n",
      "[160/200] output is tensor([[-2.5481,  2.0482],\n",
      "        [-1.5619,  0.8576]]), y is tensor([1, 1]), and loss is 0.0953 \n",
      "[180/200] output is tensor([[-2.7683,  2.2466]]), y is tensor([1]), and loss is 0.0066 \n",
      "[200/200] output is tensor([[-2.8074,  2.2436],\n",
      "        [ 2.2015, -2.8605]]), y is tensor([1, 0]), and loss is 0.0127 \n"
     ]
    }
   ],
   "source": [
    "for step in range(1, 201):\n",
    "        for b, batch in enumerate(train_iter):\n",
    "            x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "            y.data.sub_(1)  # 레이블 값을 0과 1로 변환\n",
    "            optimizer.zero_grad()\n",
    "            output = simple_model(x)\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if step % 20 == 0:\n",
    "            print(\"[{:02d}/200] output is {}, y is {}, and loss is {:.4f} \".format(step, output.data, y, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss is 0.7921548443846405, val accuracy is 62.5\n"
     ]
    }
   ],
   "source": [
    "corrects, total_loss = 0, 0\n",
    "for b, batch in enumerate(val_iter):\n",
    "    x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "    y.data.sub_(1)\n",
    "    output = simple_model(x)\n",
    "    loss = F.cross_entropy(output, y, reduction='sum')\n",
    "    total_loss += loss.item()\n",
    "    corrects += (output.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "size = len(val_iter.dataset)\n",
    "avg_loss = total_loss / size\n",
    "avg_accuracy = 100.0 * corrects / size\n",
    "print(\"val loss is {}, val accuracy is {}\".format (avg_loss, avg_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
